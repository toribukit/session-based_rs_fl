{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(f'./train_data_diginetica.npy', allow_pickle=True)\n",
    "valid = np.load(f'./test_data_diginetica.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train: 45; len test: 45\n"
     ]
    }
   ],
   "source": [
    "print(f\"len train: {len(train)}; len test: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_user = len(valid)\n",
    "max_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sessionId</th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>time</th>\n",
       "      <th>userId2</th>\n",
       "      <th>delta_t_a</th>\n",
       "      <th>delta_t_b</th>\n",
       "      <th>h_a</th>\n",
       "      <th>m_a</th>\n",
       "      <th>s_a</th>\n",
       "      <th>h_b</th>\n",
       "      <th>m_b</th>\n",
       "      <th>s_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97686</td>\n",
       "      <td>40023.0</td>\n",
       "      <td>115599</td>\n",
       "      <td>357.475</td>\n",
       "      <td>1.455462e+09</td>\n",
       "      <td>257</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97686</td>\n",
       "      <td>40023.0</td>\n",
       "      <td>79898</td>\n",
       "      <td>494.039</td>\n",
       "      <td>1.455462e+09</td>\n",
       "      <td>257</td>\n",
       "      <td>49.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97686</td>\n",
       "      <td>40023.0</td>\n",
       "      <td>35039</td>\n",
       "      <td>542.588</td>\n",
       "      <td>1.455463e+09</td>\n",
       "      <td>257</td>\n",
       "      <td>79.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97686</td>\n",
       "      <td>40023.0</td>\n",
       "      <td>11604</td>\n",
       "      <td>621.071</td>\n",
       "      <td>1.455463e+09</td>\n",
       "      <td>257</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97686</td>\n",
       "      <td>40023.0</td>\n",
       "      <td>87524</td>\n",
       "      <td>668.593</td>\n",
       "      <td>1.455463e+09</td>\n",
       "      <td>257</td>\n",
       "      <td>41.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sessionId   userId  itemId  timeframe          time  userId2  delta_t_a  \\\n",
       "0      97686  40023.0  115599    357.475  1.455462e+09      257      495.0   \n",
       "1      97686  40023.0   79898    494.039  1.455462e+09      257       49.0   \n",
       "2      97686  40023.0   35039    542.588  1.455463e+09      257       79.0   \n",
       "3      97686  40023.0   11604    621.071  1.455463e+09      257       48.0   \n",
       "4      97686  40023.0   87524    668.593  1.455463e+09      257       41.0   \n",
       "\n",
       "   delta_t_b  h_a  m_a  s_a  h_b  m_b  s_b  \n",
       "0        0.0    0    8   15    0    0    0  \n",
       "1      495.0    0    0   49    0    8   15  \n",
       "2       49.0    0    1   19    0    0   49  \n",
       "3       79.0    0    0   48    0    1   19  \n",
       "4       48.0    0    0   41    0    0   48  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all train data as one dataframe\n",
    "train_combined = np.concatenate(train)\n",
    "#convert to dataframe\n",
    "train_combined = pd.DataFrame(train_combined)\n",
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined[2].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique item IDs from the combined DataFrame\n",
    "all_unique_items = train_combined[2].unique()\n",
    "\n",
    "# Step 2: Create a universal item index mapping\n",
    "universal_item_map = pd.DataFrame({\n",
    "    'item_idx': np.arange(len(all_unique_items)),\n",
    "    'itemId': all_unique_items\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_idx</th>\n",
       "      <th>itemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>35039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>87524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>884</td>\n",
       "      <td>3694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>885</td>\n",
       "      <td>90072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>10440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>35015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888</td>\n",
       "      <td>7589.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_idx    itemId\n",
       "0           0  115599.0\n",
       "1           1   79898.0\n",
       "2           2   35039.0\n",
       "3           3   11604.0\n",
       "4           4   87524.0\n",
       "..        ...       ...\n",
       "884       884    3694.0\n",
       "885       885   90072.0\n",
       "886       886   10440.0\n",
       "887       887   35015.0\n",
       "888       888    7589.0\n",
       "\n",
       "[889 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universal_item_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, sep=',', session_key='sessionId', item_key='itemId', time_key='time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
    "        # Read csv\n",
    "        #self.df = pd.read_csv(path, sep=sep, dtype={session_key: int, item_key: int, time_key: float})\n",
    "        self.df = path\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "\n",
    "        # Add colummn item index to data\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\"\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # type is numpy.ndarray\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            # Build itemmap is a DataFrame that have 2 columns (self.item_key, 'item_idx)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
    "                                   'item_idx': item2idx[item_ids].values})\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        \"\"\"\n",
    "        self.df[self.session_key] return a set of session_key\n",
    "        self.df[self.session_key].nunique() return the size of session_key set (int)\n",
    "        self.df.groupby(self.session_key).size() return the size of each session_id\n",
    "        self.df.groupby(self.session_key).size().cumsum() retunn cumulative sum\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.session_idx_arr)\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.itemmap[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "\n",
    "        Yields:\n",
    "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = []  # indicator for the sessions to be terminated\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            idx_target = df.item_idx.values[start]\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, mask\n",
    "\n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOP1_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1_max, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n",
    "        return loss\n",
    "\n",
    "class TOP1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1Loss, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, loss_type='TOP1', use_cuda=False):\n",
    "        \"\"\" An abstract loss function that can supports custom loss functions compatible with PyTorch.\"\"\"\n",
    "        super(LossFunction, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.use_cuda = use_cuda\n",
    "        if loss_type == 'TOP1-max':\n",
    "            self._loss_fn = TOP1_max()\n",
    "        elif loss_type == 'TOP1':\n",
    "            self._loss_fn = TOP1Loss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, logit):\n",
    "        return self._loss_fn(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
    "                 dropout_hidden=.5, dropout_input=0, batch_size=1, embedding_dim=-1, use_cuda=False):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.create_final_activation(final_act)\n",
    "        if self.embedding_dim != -1:\n",
    "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
    "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        else:\n",
    "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def create_final_activation(self, final_act):\n",
    "        if final_act == 'tanh':\n",
    "            self.final_activation = nn.Tanh()\n",
    "        elif final_act == 'relu':\n",
    "            self.final_activation = nn.ReLU()\n",
    "        elif final_act == 'softmax':\n",
    "            self.final_activation = nn.Softmax()\n",
    "        elif final_act == 'softmax_logit':\n",
    "            self.final_activation = nn.LogSoftmax()\n",
    "        elif final_act.startswith('elu-'):\n",
    "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
    "        elif final_act.startswith('leaky-'):\n",
    "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input (B,): a batch of item indices from a session-parallel mini-batch.\n",
    "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\n",
    "            hidden: GRU hidden state\n",
    "        '''\n",
    "\n",
    "        if self.embedding_dim == -1:\n",
    "            embedded = self.onehot_encode(input)\n",
    "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
    "            embedded = embedded.unsqueeze(0)\n",
    "        else:\n",
    "            embedded = input.unsqueeze(0)\n",
    "            embedded = self.look_up(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        '''\n",
    "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\n",
    "        '''\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, input):\n",
    "        \"\"\"\n",
    "        Returns a one-hot vector corresponding to the input\n",
    "        Args:\n",
    "            input (B,): torch.LongTensor of item indices\n",
    "            buffer (B,output_size): buffer that stores the one-hot vector\n",
    "        Returns:\n",
    "            one_hot (B,C): torch.FloatTensor of one-hot vectors\n",
    "        \"\"\"\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = input.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, input):\n",
    "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        input = input * mask\n",
    "        return input\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''\n",
    "        Initialize the hidden state of the GRU\n",
    "        '''\n",
    "        try:\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        except:\n",
    "            self.device = 'cpu'\n",
    "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params, optimizer_type='Adagrad', lr=.05,\n",
    "                 momentum=0, weight_decay=0, eps=1e-6):\n",
    "        '''\n",
    "        An abstract optimizer class for handling various kinds of optimizers.\n",
    "        You can specify the optimizer type and related parameters as you want.\n",
    "        Usage is exactly the same as an instance of torch.optim\n",
    "\n",
    "        Args:\n",
    "            params: torch.nn.Parameter. The NN parameters to optimize\n",
    "            optimizer_type: type of the optimizer to use\n",
    "            lr: learning rate\n",
    "            momentum: momentum, if needed\n",
    "            weight_decay: weight decay, if needed. Equivalent to L2 regulariztion.\n",
    "            eps: eps parameter, if needed.\n",
    "        '''\n",
    "        if optimizer_type == 'RMSProp':\n",
    "            self.optimizer = optim.RMSprop(params, lr=lr, eps=eps, weight_decay=weight_decay, momentum=momentum)\n",
    "        elif optimizer_type == 'Adagrad':\n",
    "            self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adadelta':\n",
    "            self.optimizer = optim.Adadelta(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'Adam':\n",
    "            self.optimizer = optim.Adam(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'SparseAdam':\n",
    "            self.optimizer = optim.SparseAdam(params, lr=lr, eps=eps)\n",
    "        elif optimizer_type == 'SGD':\n",
    "            self.optimizer = optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model):\n",
    "    global sigma\n",
    "    if sigma is not None:\n",
    "        for p in model.parameters():\n",
    "            if sigma != -1 and sigma != -2:\n",
    "                sigma = sigma\n",
    "                p.data.uniform_(-sigma, sigma)\n",
    "            elif len(list(p.size())) > 1:\n",
    "                sigma = np.sqrt(6.0 / (p.size(0) + p.size(1)))\n",
    "                if sigma == -1:\n",
    "                    p.data.uniform_(-sigma, sigma)\n",
    "                else:\n",
    "                    p.data.uniform_(0, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets): #recall --> wether next item in session is within top K=20 recommended items or not\n",
    "    \"\"\"\n",
    "    Calculates the recall score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "    \"\"\"\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(indices, targets): #Mean Receiprocal Rank --> Average of rank of next item in the session.\n",
    "    \"\"\"\n",
    "    Calculates the MRR score for the given predictions and targets\n",
    "    Args:\n",
    "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "    Returns:\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    tmp = targets.view(-1, 1)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(indices, targets, k=2):\n",
    "    \"\"\"\n",
    "    Evaluates the model using Recall@K, MRR@K scores.\n",
    "\n",
    "    Args:\n",
    "        logits (B,C): torch.LongTensor. The predicted logit for the next items.\n",
    "        targets (B): torch.LongTensor. actual target indices.\n",
    "\n",
    "    Returns:\n",
    "        recall (float): the recall score\n",
    "        mrr (float): the mrr score\n",
    "    \"\"\"\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    recall = get_recall(indices, targets)\n",
    "    mrr = get_mrr(indices, targets)\n",
    "    return recall, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(object):\n",
    "    def __init__(self, model, loss_func, use_cuda, k=2):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.topk = k\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        #self.device = torch.device('cpu')\n",
    "\n",
    "    def eval(self, eval_data, batch_size):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        mrrs = []\n",
    "        dataloader = DataLoader(eval_data, batch_size)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.model.init_hidden()\n",
    "            for ii, (input, target, mask) in enumerate(dataloader):\n",
    "            #for input, target, mask in dataloader:\n",
    "                input = input.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                logit, hidden = self.model(input, hidden)\n",
    "                logit_sampled = logit[:, target.view(-1)]\n",
    "                loss = self.loss_func(logit_sampled)\n",
    "                recall, mrr = evaluate(logit, target, k=self.topk)\n",
    "\n",
    "                # torch.Tensor.item() to get a Python number from a tensor containing a single value\n",
    "                losses.append(loss.item())\n",
    "                recalls.append(recall)\n",
    "                mrrs.append(mrr.cpu())\n",
    "        mean_losses = np.mean(losses)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_mrr = np.mean(mrrs)\n",
    "        #mean_mrr = 0\n",
    "        results = {\n",
    "            'recall': mean_recall,\n",
    "            'mrr': mean_mrr\n",
    "        }\n",
    "\n",
    "        return mean_losses, mean_recall, mean_mrr, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size, clientID = 0):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.optim = optim\n",
    "        self.loss_func = loss_func\n",
    "        self.evaluation = Evaluation(self.model, self.loss_func, use_cuda, k=2)\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        #self.device = torch.device('cpu')\n",
    "        self.batch_size = batch_size\n",
    "        self.clientID = clientID\n",
    "        #self.args = args\n",
    "\n",
    "    def train(self, start_epoch, end_epoch, start_time=None):\n",
    "        if start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            self.start_time = start_time\n",
    "\n",
    "        for epoch in range(start_epoch, end_epoch + 1):\n",
    "            st = time.time()\n",
    "            print('Start Epoch #', self.clientID)\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            loss, recall, mrr, _ = self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "\n",
    "\n",
    "            print(\"client: {}, train loss: {:.4f}, loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}, time: {}\".format(self.clientID, train_loss, loss, recall, mrr, time.time() - st))\n",
    "            checkpoint = {\n",
    "                'model': self.model,\n",
    "                'epoch': epoch,\n",
    "                'optim': self.optim,\n",
    "                'loss': loss,\n",
    "                'recall': recall,\n",
    "                'mrr': mrr\n",
    "            }\n",
    "            #model_name = os.path.join('checkpoint', \"model_{0:05d}.pt\".format(epoch))\n",
    "            #torch.save(checkpoint, model_name)\n",
    "            #print(\"Save model as %s\" % model_name)\n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        def reset_hidden(hidden, mask):\n",
    "            \"\"\"Helper function that resets hidden state when some sessions terminate\"\"\"\n",
    "            if len(mask) != 0:\n",
    "                hidden[:, mask, :] = 0\n",
    "            return hidden\n",
    "\n",
    "        hidden = self.model.init_hidden()\n",
    "        dataloader = DataLoader(self.train_data, self.batch_size)\n",
    "        #for ii,(data,label) in tqdm(enumerate(train_dataloader),total=len(train_data)):\n",
    "        for ii, (input, target, mask) in enumerate(dataloader):\n",
    "            input = input.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            self.optim.zero_grad()\n",
    "            hidden = reset_hidden(hidden, mask).detach()\n",
    "            logit, hidden = self.model(input, hidden)\n",
    "            # output sampling\n",
    "            logit_sampled = logit[:, target.view(-1)]\n",
    "            loss = self.loss_func(logit_sampled)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "        mean_losses = np.mean(losses)\n",
    "        return mean_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 889\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "output_size = input_size\n",
    "batch_size = 2\n",
    "dropout_input = 0\n",
    "dropout_hidden = 0\n",
    "embedding_dim = -1\n",
    "final_act = 'tanh'\n",
    "loss_type = 'TOP1-max'\n",
    "optimizer_type = 'Adagrad'\n",
    "lr = 0.05\n",
    "weight_decay = 0\n",
    "momentum = 0\n",
    "eps = 1e-6\n",
    "n_epochs = 20\n",
    "time_sort = False\n",
    "sigma = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)\n",
    "torch.manual_seed(420)\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client():\n",
    "  def __init__(self, client_config:dict):\n",
    "    # client config as dict to make configuration dynamic\n",
    "    self.id = client_config[\"id\"]\n",
    "    self.config = client_config\n",
    "    self.__model = None\n",
    "    # self.optimizer = Optimizer(self.model.parameters(), optimizer_type=optimizer_type, lr=lr, weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "    self.loss_function = LossFunction(loss_type=loss_type, use_cuda=USE_CUDA)\n",
    "\n",
    "    # check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "      self.device = 'cuda'\n",
    "    else:\n",
    "       self.device = 'cpu'\n",
    "\n",
    "    self.train_loader = self.config[\"train_data\"]\n",
    "    self.valid_loader = self.config[\"test_data\"]\n",
    "\n",
    "  @property\n",
    "  def model(self):\n",
    "    return self.__model\n",
    "\n",
    "  @model.setter\n",
    "  def model(self, model):\n",
    "    self.__model = model\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"Return a total size of the client's local data.\"\"\"\n",
    "    return len(self.train_loader.sampler)\n",
    "\n",
    "  def train(self):\n",
    "    optimizer = Optimizer(self.model.parameters(), optimizer_type=optimizer_type, lr=lr, weight_decay=weight_decay, momentum=momentum, eps=eps)\n",
    "    trainer = Trainer(self.model, train_data=self.train_loader, eval_data=self.valid_loader, optim=optimizer, use_cuda=USE_CUDA, loss_func=self.loss_function, batch_size=batch_size, clientID=self.id)\n",
    "    trainer.train(0, 0)\n",
    "\n",
    "  def test(self):\n",
    "    evaluation = Evaluation(self.model, self.loss_function, use_cuda= USE_CUDA, k = 5)\n",
    "    loss, _, _, result = evaluation.eval(self.valloader, batch_size)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg():\n",
    "  def __init__(self):\n",
    "    self.globalmodel = GRU4REC(input_size, hidden_size, output_size, final_act=final_act,\n",
    "                            num_layers=num_layers, use_cuda=USE_CUDA, batch_size=batch_size,\n",
    "                            dropout_input=dropout_input, dropout_hidden=dropout_hidden, embedding_dim=embedding_dim)\n",
    "    self.loss_function = LossFunction(loss_type=loss_type, use_cuda=USE_CUDA)\n",
    "    self.rounds = 0\n",
    "    self.params = {}\n",
    "    init_model(self.globalmodel)\n",
    "\n",
    "    # check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "      self.device = 'cuda'\n",
    "    else:\n",
    "       self.device = 'cpu'\n",
    "\n",
    "\n",
    "  def aggregate(self, round):\n",
    "    #v1:update the aggregate to save the model with round and date indicator\n",
    "    modelparams = []\n",
    "    for i in self.params.keys():\n",
    "      modelparams.append(self.params[i])\n",
    "\n",
    "    avg_weights = {}\n",
    "    for name in modelparams[0].keys():\n",
    "      avg_weights[name] = torch.mean(torch.stack([w[name] for w in modelparams]), dim = 0)\n",
    "\n",
    "    self.globalmodel.load_state_dict(avg_weights)\n",
    "\n",
    "    #current timestamp\n",
    "    current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # filename = f\"{path_glob_m}/global_model_round_{round}_{current_time}.pth\"\n",
    "    # torch.save(self.globalmodel.state_dict(), filename)\n",
    "\n",
    "  def clientstrain(self, clientconfig):\n",
    "    clients = clientconfig\n",
    "    for i in clients.keys():\n",
    "      test_client = Client(clients[i])\n",
    "      test_client.model = copy.deepcopy(self.globalmodel)\n",
    "      test_client.train()\n",
    "      # test_client.test()\n",
    "      self.params[i] = test_client.model.state_dict()\n",
    "\n",
    "  def initiate_FL(self, clientconfig, serverdata):\n",
    "    clients = clientconfig\n",
    "    print(\"Round: {}\".format(self.rounds))\n",
    "\n",
    "    print(\"Obtaining Weights!!\")\n",
    "    self.clientstrain(clients)\n",
    "\n",
    "    #### Aggregate model\n",
    "    print(\"Aggregating Model!!\")\n",
    "    self.aggregate(self.rounds)\n",
    "\n",
    "    #### Replace parameters with global model parameters\n",
    "    for i in self.params.keys():\n",
    "        self.params[i] = self.globalmodel.state_dict()\n",
    "\n",
    "\n",
    "    servertest = serverdata\n",
    "    evaluation = Evaluation(self.globalmodel, self.loss_function, use_cuda= USE_CUDA, k = 5)\n",
    "    loss, _, _, results = evaluation.eval(servertest, batch_size)\n",
    "    # loss, results = test(self.globalmodel, servertest, device = self.device)\n",
    "    print(\"Round {} metrics:\".format(self.rounds))\n",
    "    print(\"Server Loss = {}\".format(loss))\n",
    "    print(\"Server Recall = {}\".format(results['recall']))\n",
    "    print(\"Round {} finished!\".format(self.rounds))\n",
    "    self.rounds += 1\n",
    "    return clients, results['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "numrounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client: 0\n",
      "Number of batches in the dataloader train: 5\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 1\n",
      "Number of batches in the dataloader train: 7\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 2\n",
      "Number of batches in the dataloader train: 5\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 3\n",
      "Number of batches in the dataloader train: 16\n",
      "Number of batches in the dataloader test: 3\n",
      "client: 4\n",
      "Number of batches in the dataloader train: 7\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 5\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 6\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 7\n",
      "Number of batches in the dataloader train: 2\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 8\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 9\n",
      "Number of batches in the dataloader train: 5\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 10\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 11\n",
      "Number of batches in the dataloader train: 8\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 12\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 13\n",
      "Number of batches in the dataloader train: 12\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 14\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 15\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 16\n",
      "Number of batches in the dataloader train: 2\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 17\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 18\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 19\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 20\n",
      "Number of batches in the dataloader train: 5\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 21\n",
      "Number of batches in the dataloader train: 1\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 22\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 23\n",
      "Number of batches in the dataloader train: 21\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 24\n",
      "Number of batches in the dataloader train: 6\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 25\n",
      "Number of batches in the dataloader train: 2\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 26\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 27\n",
      "Number of batches in the dataloader train: 12\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 28\n",
      "Number of batches in the dataloader train: 7\n",
      "Number of batches in the dataloader test: 4\n",
      "client: 29\n",
      "Number of batches in the dataloader train: 10\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 30\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 31\n",
      "Number of batches in the dataloader train: 6\n",
      "Number of batches in the dataloader test: 3\n",
      "client: 32\n",
      "Number of batches in the dataloader train: 6\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 33\n",
      "Number of batches in the dataloader train: 7\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 34\n",
      "Number of batches in the dataloader train: 8\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 35\n",
      "Number of batches in the dataloader train: 6\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 36\n",
      "Number of batches in the dataloader train: 2\n",
      "Number of batches in the dataloader test: 2\n",
      "client: 37\n",
      "Number of batches in the dataloader train: 1\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 38\n",
      "Number of batches in the dataloader train: 6\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 39\n",
      "Number of batches in the dataloader train: 4\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 40\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 41\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 42\n",
      "Number of batches in the dataloader train: 23\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 43\n",
      "Number of batches in the dataloader train: 9\n",
      "Number of batches in the dataloader test: 1\n",
      "client: 44\n",
      "Number of batches in the dataloader train: 3\n",
      "Number of batches in the dataloader test: 1\n",
      "Round: 0\n",
      "Obtaining Weights!!\n",
      "Start Epoch # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tori_rs\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1102: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m allrecall \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(numrounds):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   clients, recall \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49minitiate_FL(clients, serverdata)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   allrecall\u001b[39m.\u001b[39mappend(recall)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRound: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mObtaining Weights!!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclientstrain(clients)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m#### Aggregate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAggregating Model!!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m test_client \u001b[39m=\u001b[39m Client(clients[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m test_client\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobalmodel)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m test_client\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# test_client.test()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[i] \u001b[39m=\u001b[39m test_client\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict()\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Optimizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), optimizer_type\u001b[39m=\u001b[39moptimizer_type, lr\u001b[39m=\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mweight_decay, momentum\u001b[39m=\u001b[39mmomentum, eps\u001b[39m=\u001b[39meps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, train_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader, eval_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_loader, optim\u001b[39m=\u001b[39moptimizer, use_cuda\u001b[39m=\u001b[39mUSE_CUDA, loss_func\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function, batch_size\u001b[39m=\u001b[39mbatch_size, clientID\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStart Epoch #\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclientID)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epoch(epoch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss, recall, mrr, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation\u001b[39m.\u001b[39;49meval(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclient: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, train loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, recall: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, mrr: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, time: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclientID, train_loss, loss, recall, mrr, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m st))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmrr\u001b[39m\u001b[39m'\u001b[39m: mrr\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m }\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minit_hidden()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m ii, (\u001b[39minput\u001b[39m, target, mask) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m#for input, target, mask in dataloader:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m iters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m maxiter \u001b[39m=\u001b[39m iters\u001b[39m.\u001b[39mmax()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m start \u001b[39m=\u001b[39m click_offsets[session_idx_arr[iters]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m end \u001b[39m=\u001b[39m click_offsets[session_idx_arr[iters] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m mask \u001b[39m=\u001b[39m []  \u001b[39m# indicator for the sessions to be terminated\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "clients = {}\n",
    "\n",
    "for i in range(max_user):\n",
    "  clients[i] = {\"id\": i, \"val_size\": 0.25, \"batch_size\": batch_size, \"local_epoch\": 1}\n",
    "  clients[i]['train_data'] = Dataset(train[i], itemmap=universal_item_map)\n",
    "  clients[i]['test_data'] = Dataset(valid[i], itemmap=universal_item_map)\n",
    "  print(f\"client: {i}\")\n",
    "  print(f\"Number of batches in the dataloader train: {len(clients[i]['train_data'])}\")\n",
    "  print(f\"Number of batches in the dataloader test: {len(clients[i]['test_data'])}\")\n",
    "\n",
    "serverdata = Dataset(valid[37])\n",
    "server = FedAvg() ### initialize server\n",
    "\n",
    "allrecall = []\n",
    "for i in range(numrounds):\n",
    "  clients, recall = server.initiate_FL(clients, serverdata)\n",
    "  allrecall.append(recall)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Recall of all rounds: {}\".format(allrecall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients 0 | Recall: 0.00 | Loss: 1.0008\n",
      "--------------------------------------------------\n",
      "Clients 1 | Recall: 0.00 | Loss: 1.0001\n",
      "--------------------------------------------------\n",
      "Clients 2 | Recall: 0.50 | Loss: 1.0022\n",
      "--------------------------------------------------\n",
      "Clients 3 | Recall: 0.00 | Loss: 1.0014\n",
      "--------------------------------------------------\n",
      "Clients 4 | Recall: 0.00 | Loss: 1.0027\n",
      "--------------------------------------------------\n",
      "Clients 5 | Recall: 0.00 | Loss: 1.0001\n",
      "--------------------------------------------------\n",
      "Clients 6 | Recall: 0.00 | Loss: 1.0013\n",
      "--------------------------------------------------\n",
      "Clients 7 | Recall: 0.00 | Loss: 1.0006\n",
      "--------------------------------------------------\n",
      "Clients 8 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 9 | Recall: 0.00 | Loss: 1.0003\n",
      "--------------------------------------------------\n",
      "Clients 10 | Recall: 0.00 | Loss: 1.0006\n",
      "--------------------------------------------------\n",
      "Clients 11 | Recall: 0.00 | Loss: 1.0002\n",
      "--------------------------------------------------\n",
      "Clients 12 | Recall: 0.00 | Loss: 1.0013\n",
      "--------------------------------------------------\n",
      "Clients 13 | Recall: 0.00 | Loss: 1.0014\n",
      "--------------------------------------------------\n",
      "Clients 14 | Recall: 0.00 | Loss: 1.0011\n",
      "--------------------------------------------------\n",
      "Clients 15 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 16 | Recall: 0.00 | Loss: 1.0022\n",
      "--------------------------------------------------\n",
      "Clients 17 | Recall: 0.00 | Loss: 1.0003\n",
      "--------------------------------------------------\n",
      "Clients 18 | Recall: 0.00 | Loss: 1.0003\n",
      "--------------------------------------------------\n",
      "Clients 19 | Recall: 0.00 | Loss: 1.0000\n",
      "--------------------------------------------------\n",
      "Clients 20 | Recall: 0.00 | Loss: 1.0001\n",
      "--------------------------------------------------\n",
      "Clients 21 | Recall: 0.00 | Loss: 1.0002\n",
      "--------------------------------------------------\n",
      "Clients 22 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 23 | Recall: 0.00 | Loss: 1.0014\n",
      "--------------------------------------------------\n",
      "Clients 24 | Recall: 0.00 | Loss: 1.0014\n",
      "--------------------------------------------------\n",
      "Clients 25 | Recall: 0.00 | Loss: 1.0002\n",
      "--------------------------------------------------\n",
      "Clients 26 | Recall: 0.00 | Loss: 1.0021\n",
      "--------------------------------------------------\n",
      "Clients 27 | Recall: 0.00 | Loss: 1.0006\n",
      "--------------------------------------------------\n",
      "Clients 28 | Recall: 0.00 | Loss: 1.0008\n",
      "--------------------------------------------------\n",
      "Clients 29 | Recall: 0.00 | Loss: 1.0001\n",
      "--------------------------------------------------\n",
      "Clients 30 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 31 | Recall: 0.00 | Loss: 1.0010\n",
      "--------------------------------------------------\n",
      "Clients 32 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 33 | Recall: 0.00 | Loss: 1.0000\n",
      "--------------------------------------------------\n",
      "Clients 34 | Recall: 0.00 | Loss: 1.0002\n",
      "--------------------------------------------------\n",
      "Clients 35 | Recall: 0.00 | Loss: 1.0004\n",
      "--------------------------------------------------\n",
      "Clients 36 | Recall: 0.00 | Loss: 1.0011\n",
      "--------------------------------------------------\n",
      "Clients 37 | Recall: 0.00 | Loss: 1.0010\n",
      "--------------------------------------------------\n",
      "Clients 38 | Recall: 0.00 | Loss: 1.0024\n",
      "--------------------------------------------------\n",
      "Clients 39 | Recall: 0.00 | Loss: 1.0002\n",
      "--------------------------------------------------\n",
      "Clients 40 | Recall: 0.00 | Loss: 1.0009\n",
      "--------------------------------------------------\n",
      "Clients 41 | Recall: 0.00 | Loss: 1.0003\n",
      "--------------------------------------------------\n",
      "Clients 42 | Recall: 0.00 | Loss: 1.0008\n",
      "--------------------------------------------------\n",
      "Clients 43 | Recall: 0.00 | Loss: 1.0007\n",
      "--------------------------------------------------\n",
      "Clients 44 | Recall: 0.00 | Loss: 1.0000\n",
      "--------------------------------------------------\n",
      "Average Recall: 0.01 | Average Loss: 1.0008\n"
     ]
    }
   ],
   "source": [
    "final_model = server.globalmodel\n",
    "recall_clients = []\n",
    "loss_clients = []\n",
    "\n",
    "for i in range(max_user):\n",
    "    evaluation = Evaluation(final_model, server.loss_function, use_cuda= USE_CUDA, k = 5)\n",
    "    loss, _, _, results = evaluation.eval(clients[i]['test_data'], batch_size)\n",
    "    recall_clients.append(results['recall'])\n",
    "    loss_clients.append(loss)\n",
    "\n",
    "    print(f\"Clients {i} | Recall: {results['recall']:.2f} | Loss: {loss:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"Average Recall: {np.mean(recall_clients):.2f} | Average Loss: {np.mean(loss_clients):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
