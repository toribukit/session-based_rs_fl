{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 889\n",
    "hidden_size = 100\n",
    "output_size = input_size\n",
    "batch_size = 8\n",
    "lr = 0.01\n",
    "numrounds = 5\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(f'./train_data_diginetica.npy', allow_pickle=True)\n",
    "valid_data = np.load(f'./test_data_diginetica.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sessionId', 'userId', 'itemId', 'timeframe', 'time', 'userId2',\n",
       "       'delta_t_a', 'delta_t_b', 'h_a', 'm_a', 's_a', 'h_b', 'm_b', 's_b'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train: 45; len test: 45\n"
     ]
    }
   ],
   "source": [
    "print(f\"len train: {len(train_data)}; len test: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_user = len(valid_data)\n",
    "max_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all train data as one dataframe\n",
    "train_combined = np.concatenate(train_data)\n",
    "#convert to dataframe\n",
    "train_combined = pd.DataFrame(train_combined)\n",
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined[2].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique item IDs from the combined DataFrame\n",
    "all_unique_items = train_combined[2].unique()\n",
    "\n",
    "# Step 2: Create a universal item index mapping\n",
    "universal_item_map = pd.DataFrame({\n",
    "    'item_idx': np.arange(len(all_unique_items)),\n",
    "    'itemId': all_unique_items\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_idx</th>\n",
       "      <th>itemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>35039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>87524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>884</td>\n",
       "      <td>3694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>885</td>\n",
       "      <td>90072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>10440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>35015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888</td>\n",
       "      <td>7589.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_idx    itemId\n",
       "0           0  115599.0\n",
       "1           1   79898.0\n",
       "2           2   35039.0\n",
       "3           3   11604.0\n",
       "4           4   87524.0\n",
       "..        ...       ...\n",
       "884       884    3694.0\n",
       "885       885   90072.0\n",
       "886       886   10440.0\n",
       "887       887   35015.0\n",
       "888       888    7589.0\n",
       "\n",
       "[889 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universal_item_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, sep=',', session_key='sessionId', item_key='itemId', time_key='time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
    "        # Read csv\n",
    "        #self.df = pd.read_csv(path, sep=sep, dtype={session_key: int, item_key: int, time_key: float})\n",
    "        self.df = path\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "\n",
    "        # Add colummn item index to data\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        \"\"\"\n",
    "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "        \"\"\"\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "\n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\"\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # type is numpy.ndarray\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            # Build itemmap is a DataFrame that have 2 columns (self.item_key, 'item_idx)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
    "                                   'item_idx': item2idx[item_ids].values})\n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "    def get_click_offset(self):\n",
    "        \"\"\"\n",
    "        self.df[self.session_key] return a set of session_key\n",
    "        self.df[self.session_key].nunique() return the size of session_key set (int)\n",
    "        self.df.groupby(self.session_key).size() return the size of each session_id\n",
    "        self.df.groupby(self.session_key).size().cumsum() retunn cumulative sum\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "\n",
    "    def order_session_idx(self):\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.session_idx_arr)\n",
    "\n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.itemmap[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDataset(Dataset):\n",
    "    def __init__(self, data, itemmap, session_key='sessionId', item_key='itemId', time_key='time'):\n",
    "        self.data = data\n",
    "        self.itemmap = itemmap\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "        # Map items to indices\n",
    "        self.data = pd.merge(self.data, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "        # Sort by session and time\n",
    "        self.data.sort_values([self.session_key, self.time_key], inplace=True)\n",
    "\n",
    "        # Group data by session and collect item indices\n",
    "        self.sessions = self.data.groupby(self.session_key)['item_idx'].apply(list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        session_items = self.sessions.iloc[index]\n",
    "        sequence = torch.tensor(session_items[:-1], dtype=torch.long)\n",
    "        target = torch.tensor(session_items[1:], dtype=torch.long)\n",
    "        return sequence, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataLoader():\n",
    "#     def __init__(self, dataset, batch_size=1):\n",
    "#         \"\"\"\n",
    "#         A class for creating session-parallel mini-batches.\n",
    "\n",
    "#         Args:\n",
    "#              dataset (SessionDataset): the session dataset to generate the batches from\n",
    "#              batch_size (int): size of the batch\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "\n",
    "#         Yields:\n",
    "#             input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
    "#             target (B,): a Variable that stores the target item indices\n",
    "#             masks: Numpy array indicating the positions of the sessions to be terminated\n",
    "#         \"\"\"\n",
    "#         # initializations\n",
    "#         df = self.dataset.df\n",
    "#         click_offsets = self.dataset.click_offsets\n",
    "#         session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "#         iters = np.arange(self.batch_size)\n",
    "#         maxiter = iters.max()\n",
    "#         start = click_offsets[session_idx_arr[iters]]\n",
    "#         end = click_offsets[session_idx_arr[iters] + 1]\n",
    "#         mask = []  # indicator for the sessions to be terminated\n",
    "#         finished = False\n",
    "\n",
    "#         while not finished:\n",
    "#             minlen = (end - start).min()\n",
    "#             # Item indices(for embedding) for clicks where the first sessions start\n",
    "#             idx_target = df.item_idx.values[start]\n",
    "\n",
    "#             for i in range(minlen - 1):\n",
    "#                 # Build inputs & targets\n",
    "#                 idx_input = idx_target\n",
    "#                 idx_target = df.item_idx.values[start + i + 1]\n",
    "#                 input = torch.LongTensor(idx_input)\n",
    "#                 target = torch.LongTensor(idx_target)\n",
    "#                 yield input, target, mask\n",
    "\n",
    "#             # click indices where a particular session meets second-to-last element\n",
    "#             start = start + (minlen - 1)\n",
    "#             # see if how many sessions should terminate\n",
    "#             mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "#             for idx in mask:\n",
    "#                 maxiter += 1\n",
    "#                 if maxiter >= len(click_offsets) - 1:\n",
    "#                     finished = True\n",
    "#                     break\n",
    "#                 # update the next starting/ending point\n",
    "#                 iters[idx] = maxiter\n",
    "#                 start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "#                 end[idx] = click_offsets[session_idx_arr[maxiter] + 1]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # Return the number of batches in the dataset\n",
    "#         return (len(self.dataset) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-1)\n",
    "    return sequences_padded, targets_padded\n",
    "\n",
    "def get_loader(data, itemmap, batch_size=32, shuffle=True):\n",
    "    dataset = GRUDataset(data, itemmap=itemmap)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the GRU model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of expected features in the input `x`\n",
    "            hidden_size (int): The number of features in the hidden state `h`\n",
    "            output_size (int): The size of the output layer (number of items)\n",
    "            num_layers (int, optional): Number of recurrent layers. Default: 1\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x: Input data\n",
    "            hidden: Hidden state\n",
    "\n",
    "        Returns:\n",
    "            Output and new hidden state\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # GRU\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # Predict next item\n",
    "        output = self.fc(output[:, -1, :])\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state of the GRU.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The size of the batch\n",
    "\n",
    "        Returns:\n",
    "            Initial hidden state\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TOP1MaxLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TOP1MaxLoss, self).__init__()\n",
    "\n",
    "    def forward(self, scores, targets):\n",
    "        # Remove padding (-1) from targets\n",
    "        mask = (targets != -1)\n",
    "        targets = targets[mask]\n",
    "        scores = scores[mask]\n",
    "\n",
    "        # Get the positive predictions\n",
    "        pos_pred = scores.gather(1, targets.view(-1, 1)).squeeze()\n",
    "\n",
    "        # Calculate the difference between the positive predictions and all other predictions\n",
    "        diff = -torch.sigmoid(pos_pred.unsqueeze(1) - scores)\n",
    "\n",
    "        # Exclude the positive items from the loss\n",
    "        diff.scatter_(1, targets.view(-1, 1), 0)\n",
    "\n",
    "        # Calculate the TOP1-max loss\n",
    "        loss = torch.mean(torch.max(diff, dim=1)[0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, device, k):\n",
    "    \"\"\"Evaluate the network on the given data loader for top-k recommendation.\"\"\"\n",
    "    net.eval()\n",
    "    total_recall = 0.0\n",
    "    total_mrr = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            data, target = x.to(device), y.to(device)\n",
    "            hidden = net.init_hidden(data.size(0))\n",
    "            outputs, _ = net(data, hidden)\n",
    "\n",
    "            # Select top-k items\n",
    "            _, top_k_indices = torch.topk(outputs, k, dim=1)\n",
    "\n",
    "            # Calculate recall and MRR for each batch\n",
    "            for i in range(data.size(0)):\n",
    "                target_item = target[i].item()\n",
    "                top_k_items = top_k_indices[i].tolist()\n",
    "\n",
    "                # Calculate Recall@k\n",
    "                if target_item in top_k_items:\n",
    "                    total_recall += 1\n",
    "\n",
    "                # Calculate MRR@k\n",
    "                if target_item in top_k_items:\n",
    "                    rank = top_k_items.index(target_item)\n",
    "                    total_mrr += 1 / (rank + 1)\n",
    "\n",
    "            total_count += data.size(0)\n",
    "\n",
    "    avg_recall = total_recall / total_count\n",
    "    avg_mrr = total_mrr / total_count\n",
    "    return avg_recall, avg_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs, device, valloader=None):\n",
    "    \"\"\"Train the network for session-based recommendation.\"\"\"\n",
    "    # Define loss and optimizer\n",
    "    criterion = TOP1MaxLoss()  # Replace with your loss function\n",
    "    optimizer = torch.optim.Adagrad(net.parameters(), lr= lr)\n",
    "\n",
    "    print(f\"Training {epochs} epoch(s) w/ {len(trainloader)} batches each\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in trainloader:\n",
    "            data, target = x.to(device), y.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            # print(data.shape)\n",
    "            # print(target.shape)\n",
    "            hidden = net.init_hidden(data.size(0))\n",
    "            outputs, _ = net(data, hidden)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        recall, mrr = evaluate(net, valloader, device, k=3)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss: {total_loss / len(trainloader):.4f}, Recall: {recall:.4f}, MRR: {mrr:.4f}\")\n",
    "\n",
    "        net.train()  # Ensure the network is in training mode\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    net.to(\"cpu\")  # Move model back to CPU\n",
    "\n",
    "    print(f\"Training completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5 epoch(s) w/ 1 batches each\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 889] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica_2.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m net \u001b[39m=\u001b[39m GRUModel(input_size, hidden_size, output_size, num_layers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train(net, trainloader, numrounds, DEVICE, testloader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Evaluate the network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m recall, mrr \u001b[39m=\u001b[39m evaluate(net, testloader, DEVICE, k\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica_2.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs, _ \u001b[39m=\u001b[39m net(data, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Compute loss and backpropagate\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tori_rs\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\tori_rs\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_gru_diginetica_2.ipynb Cell 27\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mask \u001b[39m=\u001b[39m (targets \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m targets \u001b[39m=\u001b[39m targets[mask]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m scores \u001b[39m=\u001b[39m scores[mask]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Get the positive predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_gru_diginetica_2.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pos_pred \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [5, 8] at index 1 does not match the shape of the indexed tensor [5, 889] at index 1"
     ]
    }
   ],
   "source": [
    "local_train = train_data[0]\n",
    "local_test = valid_data[0]\n",
    "\n",
    "trainloader = get_loader(local_train, itemmap=universal_item_map, batch_size=batch_size)\n",
    "testloader = get_loader(local_test, itemmap=universal_item_map, batch_size=batch_size)\n",
    "\n",
    "# Initialize the network\n",
    "net = GRUModel(input_size, hidden_size, output_size, num_layers=1)\n",
    "\n",
    "# Train the network\n",
    "train(net, trainloader, numrounds, DEVICE, testloader)\n",
    "\n",
    "# Evaluate the network\n",
    "recall, mrr = evaluate(net, testloader, DEVICE, k=20)\n",
    "\n",
    "print(f\"Recall@20: {recall:.4f}\")\n",
    "print(f\"MRR@20: {mrr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
