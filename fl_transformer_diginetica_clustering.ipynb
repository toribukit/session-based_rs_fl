{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tr_data = np.load(f'./train_data_diginetica.npy', allow_pickle=True)\n",
    "raw_val_data = np.load(f'./test_data_diginetica.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'diginetica'\n",
    "attack_type = 'B' # A1: label_poison, A2: gaussian_attack, A3: scaling_attack, A4: reverse_attack\n",
    "local_learning_rate = 0.01\n",
    "local_steps= 1\n",
    "data_path= f\".\"\n",
    "learning_rate_decay_gamma= 0.99\n",
    "learning_rate_decay= False\n",
    "future_test= False\n",
    "mu= 1\n",
    "global_rounds= 50\n",
    "num_clients= len(raw_val_data)\n",
    "join_ratio= 1.0\n",
    "attack_ratio= 0.0\n",
    "algorithm= \"FedCHAR\"\n",
    "future_ratio= 0.0\n",
    "finetune_rounds= 0\n",
    "eval_gap= 1\n",
    "detailed_info= False\n",
    "partition= \"nature\"\n",
    "initial_rounds= 10\n",
    "n_clusters= 3\n",
    "metric= 'cosine'\n",
    "linkage= 'complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter for recommender system\n",
    "input_size = 889\n",
    "hidden_size = 400\n",
    "num_layers = 3\n",
    "nhead = 4\n",
    "output_size = input_size\n",
    "batch_size = 10\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all train data as one dataframe\n",
    "train_combined = np.concatenate(raw_tr_data)\n",
    "#convert to dataframe\n",
    "train_combined = pd.DataFrame(train_combined)\n",
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique item IDs from the combined DataFrame\n",
    "all_unique_items = train_combined[2].unique()\n",
    "\n",
    "# Step 2: Create a universal item index mapping\n",
    "universal_item_map = pd.DataFrame({\n",
    "    'item_idx': np.arange(len(all_unique_items)),\n",
    "    'itemId': all_unique_items\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDataset(Dataset):\n",
    "    def __init__(self, data, itemmap, session_key='sessionId', item_key='itemId', time_key='time'):\n",
    "        self.data = data\n",
    "        self.itemmap = itemmap\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "\n",
    "        # Map items to indices\n",
    "        self.data = pd.merge(self.data, self.itemmap, on=self.item_key, how='inner')\n",
    "\n",
    "        # Sort by session and time\n",
    "        self.data.sort_values([self.session_key, self.time_key], inplace=True)\n",
    "\n",
    "        # Group data by session and collect item indices\n",
    "        self.sessions = self.data.groupby(self.session_key)['item_idx'].apply(list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        session_items = self.sessions.iloc[index]\n",
    "        sequence = torch.tensor(session_items[:-1], dtype=torch.long)\n",
    "        target = torch.tensor(session_items[1:], dtype=torch.long)\n",
    "        return sequence, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-1)\n",
    "    return sequences_padded, targets_padded\n",
    "\n",
    "def get_loader(data, itemmap, batch_size=32, shuffle=True):\n",
    "    dataset = GRUDataset(data, itemmap=itemmap)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedGradientDescent(Optimizer):\n",
    "  def __init__(self, params, lr=0.01, mu=0.0):\n",
    "    default = dict(lr=lr, mu=mu)\n",
    "    super().__init__(params, default)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def step(self, global_params, device):\n",
    "    for group in self.param_groups:\n",
    "      for p, g in zip(group['params'], global_params):\n",
    "        g = g.to(device)\n",
    "        d_p = p.grad.data + group['mu'] * (p.data - g.data)\n",
    "        p.data.add_(d_p, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, nhead=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The number of expected features in the input `x`\n",
    "            hidden_size (int): The number of features in the hidden state `h`\n",
    "            output_size (int): The size of the output layer (number of items)\n",
    "            num_layers (int, optional): Number of transformer layers. Default: 1\n",
    "            nhead (int, optional): Number of heads in the multiheadattention models. Default: 4\n",
    "            dropout (float, optional): The dropout value. Default: 0.1\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, dropout)\n",
    "\n",
    "        # Transformer layer\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(x):\n",
    "            device = x.device\n",
    "            mask = self._generate_square_subsequent_mask(len(x)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        x = self.embedding(x) * math.sqrt(self.hidden_size)\n",
    "        x = self.pos_encoder(x)\n",
    "        output = self.transformer_encoder(x, self.src_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(object):\n",
    "  \"\"\"\n",
    "  Base class for clients in federated learning.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model, id, malicious, **kwargs):\n",
    "    self.model = copy.deepcopy(model)\n",
    "    self.dataset = dataset\n",
    "    self.device = DEVICE\n",
    "    self.id = id\n",
    "    self.malicious = malicious\n",
    "    self.attack_type = attack_type\n",
    "    self.num_classes = output_size\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = local_learning_rate\n",
    "    self.local_steps = local_steps\n",
    "    self.data_path = data_path\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.future_test = future_test\n",
    "\n",
    "\n",
    "    # check BatchNorm\n",
    "    self.has_BatchNorm = False\n",
    "    for layer in self.model.children():\n",
    "      if isinstance(layer, nn.BatchNorm2d):\n",
    "        self.has_BatchNorm = True\n",
    "        break\n",
    "\n",
    "    self.loss = nn.CrossEntropyLoss(ignore_index=-1)  # Replace with your loss function\n",
    "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate) # momentum=0.9, weight_decay=1e-4\n",
    "    self.learning_rate_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "      optimizer=self.optimizer,\n",
    "      gamma=learning_rate_decay_gamma\n",
    "    )\n",
    "\n",
    "  def load_train_data(self, batch_size=None):\n",
    "    if batch_size == None:\n",
    "      batch_size = self.batch_size\n",
    "    train_data = get_loader(raw_tr_data[self.id], itemmap=universal_item_map, batch_size=batch_size)\n",
    "\n",
    "    # label poison attack\n",
    "    if self.malicious and self.attack_type == 'A1':\n",
    "      for idx in range(len(train_data)):\n",
    "        train_data[idx][1] = self.num_classes - train_data[idx][1] - 1\n",
    "    self.train_samples = len(train_data)\n",
    "    return train_data\n",
    "\n",
    "  def load_test_data(self, batch_size=None):\n",
    "    \"\"\"\n",
    "    fine-tunes the model using the loaded training data\n",
    "    \"\"\"\n",
    "    if batch_size == None:\n",
    "      batch_size = self.batch_size\n",
    "    test_data = get_loader(raw_val_data[self.id], itemmap=universal_item_map, batch_size=batch_size)\n",
    "    return test_data\n",
    "  \n",
    "  def set_parameters(self, model):\n",
    "    for new_param, old_param in zip(model.parameters(), self.model.parameters()):\n",
    "      old_param.data = new_param.data.clone()\n",
    "\n",
    "  def fine_tuning(self):\n",
    "    trainloader = self.load_train_data()\n",
    "    self.model.train()\n",
    "\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "      # if type(x) == type([]):\n",
    "      #   x[0] = x[0].to(self.device)\n",
    "      # else:\n",
    "      #   x = x.to(self.device)\n",
    "      x = x.to(self.device)\n",
    "      y = y.to(self.device)\n",
    "      self.optimizer.zero_grad()\n",
    "      output = self.model(x)\n",
    "      output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "      y = y.view(-1)  # Flatten target\n",
    "      # output = self.model(x)\n",
    "      loss = self.loss(output, y)\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "  def new_test_metrics(self):\n",
    "    \"\"\"\n",
    "    evaluates the model's performance on test data, particularly its accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    testloaderfull = self.load_test_data()\n",
    "    self.model.eval()\n",
    "\n",
    "    total_recall = 0.0\n",
    "    total_mrr = 0.0\n",
    "    test_num = 0\n",
    "    y_prob = [] #model outputs or probabilities\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x, y in testloaderfull:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        # hidden = self.model.init_hidden(x.size(0))\n",
    "        # output = self.model(x)\n",
    "        output = self.model(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "\n",
    "        # Select top-k items\n",
    "        _, top_k_indices = torch.topk(output, K, dim=1)\n",
    "\n",
    "        # test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "        # test_num += y.shape[0]\n",
    "\n",
    "        # Calculate recall and MRR for each batch\n",
    "        for i in range(x.size(0)):\n",
    "          for y_item in y[i]:\n",
    "            if y_item == -1:\n",
    "              continue\n",
    "            target_item_scalar = y_item.item()\n",
    "            top_k_items = top_k_indices[i].tolist()\n",
    "\n",
    "            # Calculate Recall@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              total_recall += 1\n",
    "\n",
    "            # Calculate MRR@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              rank = top_k_items.index(target_item_scalar)\n",
    "              total_mrr += 1 / (rank + 1)\n",
    "          \n",
    "          test_num += len(y[i][y[i] != -1])  # Count non-padding elements\n",
    "\n",
    "        y_prob.append(output.detach().cpu().numpy())\n",
    "        nc = self.num_classes\n",
    "        if self.num_classes == 2:\n",
    "          nc += 1\n",
    "        lb = label_binarize(y.detach().cpu().numpy(), classes=np.arange(nc))\n",
    "        if self.num_classes == 2:\n",
    "          lb = lb[:, :2]\n",
    "        y_true.append(lb)\n",
    "\n",
    "    y_prob = np.concatenate(y_prob, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "\n",
    "    return total_recall, total_mrr, test_num\n",
    "\n",
    "  def new_train_metrics(self):\n",
    "    \"\"\"\n",
    "    evaluates the model's loss on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    trainloader = self.load_train_data()\n",
    "    self.model.eval()\n",
    "\n",
    "    train_num = 0\n",
    "    losses = 0.0\n",
    "    with torch.no_grad():\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "        y = y.view(-1)  # Flatten target\n",
    "        # output = self.model(x)\n",
    "        # calculate losses\n",
    "        loss = self.loss(output, y)\n",
    "        train_num += y.shape[0]\n",
    "        losses += loss * y.shape[0]\n",
    "        # loss = self.loss(output, y)\n",
    "        # train_num += y.shape[0]\n",
    "        # losses += loss.item() * y.shape[0]\n",
    "\n",
    "    return losses, train_num\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    testloaderfull = self.load_test_data()\n",
    "\n",
    "    self.model.eval()\n",
    "\n",
    "    total_recall = 0.0\n",
    "    total_mrr = 0.0\n",
    "    test_num = 0\n",
    "    # y_prob = []\n",
    "    # y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x, y in testloaderfull:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy \n",
    "\n",
    "        # Select top-k items\n",
    "        _, top_k_indices = torch.topk(output, K, dim=1)\n",
    "\n",
    "        # test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "        # test_num += y.shape[0]\n",
    "\n",
    "        # Calculate recall and MRR for each batch\n",
    "        for i in range(x.size(0)):\n",
    "          for y_item in y[i]:\n",
    "            if y_item == -1:\n",
    "              continue\n",
    "            target_item_scalar = y_item.item()\n",
    "            top_k_items = top_k_indices[i].tolist()\n",
    "\n",
    "            # Calculate Recall@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              total_recall += 1\n",
    "\n",
    "            # Calculate MRR@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              rank = top_k_items.index(target_item_scalar)\n",
    "              total_mrr += 1 / (rank + 1)\n",
    "          \n",
    "          test_num += len(y[i][y[i] != -1]) # Count non-padding elements\n",
    "\n",
    "        # y_prob.append(output.detach().cpu().numpy())\n",
    "        # nc = self.num_classes\n",
    "        # if self.num_classes == 2:\n",
    "        #   nc += 1\n",
    "        # lb = label_binarize(y.detach().cpu().numpy(), classes=np.arange(nc))\n",
    "        # if self.num_classes == 2:\n",
    "        #   lb = lb[:, :2]\n",
    "        # y_true.append(lb)\n",
    "\n",
    "    # y_prob = np.concatenate(y_prob, axis=0)\n",
    "    # y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    return total_recall, total_mrr, test_num\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    trainloader = self.load_train_data()\n",
    "\n",
    "    self.model.eval()\n",
    "\n",
    "    train_num = 0\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "        y = y.view(-1)  # Flatten target\n",
    "        # output = self.model(x)\n",
    "        loss = self.loss(output, y)\n",
    "        train_num += y.shape[0]\n",
    "        losses += loss * y.shape[0]\n",
    "\n",
    "    return losses, train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clientCHAR(Client):\n",
    "  def __init__(self, model, id, malicious, **kwargs):\n",
    "    super().__init__(model, id, malicious, **kwargs)\n",
    "    self.mu = mu\n",
    "    self.model_per = copy.deepcopy(self.model)\n",
    "    self.optimizer_per = PerturbedGradientDescent(self.model_per.parameters(), lr=self.learning_rate, mu=self.mu)\n",
    "    self.learning_rate_scheduler_per = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=self.optimizer_per,\n",
    "        gamma=learning_rate_decay_gamma\n",
    "        )\n",
    "\n",
    "  def dtrain(self):\n",
    "    trainloader = self.load_train_data()\n",
    "    model = copy.deepcopy(self.model)\n",
    "    self.model.train()\n",
    "    self.model_per.train()\n",
    "\n",
    "    max_local_steps = self.local_steps\n",
    "\n",
    "    for step in range(max_local_steps):\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        out_p = self.model_per(x)\n",
    "        out_p = out_p.view(-1, out_p.size(-1))  # Flatten output for cross-entropy\n",
    "        y = y.view(-1)  # Flatten target\n",
    "        # out_p = self.model_per(x)\n",
    "        loss = self.loss(out_p, y)\n",
    "        self.optimizer_per.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_per.step(model.parameters(), self.device)\n",
    "\n",
    "        out_g = self.model(x)\n",
    "        out_g = out_g.view(-1, out_g.size(-1))  # Flatten output for cross-entropy\n",
    "        # out_g = self.model(x)\n",
    "        loss = self.loss(out_g, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    if self.learning_rate_decay:\n",
    "      self.learning_rate_scheduler.step()\n",
    "      self.learning_rate_scheduler_per.step()\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    testloaderfull = self.load_test_data()\n",
    "    self.model_per.eval()\n",
    "\n",
    "    total_recall = 0.0\n",
    "    total_mrr = 0.0\n",
    "    test_num = 0\n",
    "    # y_prob = []\n",
    "    # y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x, y in testloaderfull:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model_per(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "        # output = self.model_per(x)\n",
    "\n",
    "        # test_acc += (torch.sum(torch.argmax(output, dim=1) == y)).item()\n",
    "        # test_num += y.shape[0]\n",
    "\n",
    "        # Select top-k items\n",
    "        _, top_k_indices = torch.topk(output, K, dim=1)\n",
    "\n",
    "        # Calculate recall and MRR for each batch\n",
    "        for i in range(x.size(0)):\n",
    "          for y_item in y[i]:\n",
    "            if y_item == -1:\n",
    "              continue\n",
    "            target_item_scalar = y_item.item()\n",
    "            top_k_items = top_k_indices[i].tolist()\n",
    "\n",
    "            # Calculate Recall@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              total_recall += 1\n",
    "\n",
    "            # Calculate MRR@k\n",
    "            if target_item_scalar in top_k_items:\n",
    "              rank = top_k_items.index(target_item_scalar)\n",
    "              total_mrr += 1 / (rank + 1)\n",
    "          \n",
    "          test_num += len(y[i][y[i] != -1])\n",
    "\n",
    "    #     y_prob.append(F.softmax(output).detach().cpu().numpy())\n",
    "    #     y_true.append(label_binarize(y.detach().cpu().numpy(), classes=np.arange(self.num_classes)))\n",
    "\n",
    "    # y_prob = np.concatenate(y_prob, axis=0)\n",
    "    # y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    return total_recall, total_mrr, test_num\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    trainloader = self.load_train_data()\n",
    "    self.model_per.eval()\n",
    "\n",
    "    train_num = 0\n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "      for x, y in trainloader:\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model_per(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "        y = y.view(-1) # Flatten target\n",
    "        # output = self.model_per(x)\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        #add a regularization term to the loss\n",
    "        # ensure that the personalized model doesn't deviate too far from the global model.\n",
    "        # The strength of this regularization is controlled by the parameter self.mu\n",
    "        gm = torch.cat([p.data.view(-1) for p in self.model.parameters()], dim=0)\n",
    "        pm = torch.cat([p.data.view(-1) for p in self.model_per.parameters()], dim=0)\n",
    "        loss += 0.5 * self.mu * torch.norm(pm-gm, p=2) #element-wise difference using L2 norm\n",
    "\n",
    "        train_num += y.shape[0]\n",
    "        losses += loss.item() * y.shape[0]\n",
    "\n",
    "    return losses, train_num\n",
    "\n",
    "  def get_update(self, global_model):\n",
    "    trainloader = self.load_train_data()\n",
    "    model = copy.deepcopy(self.model) #old model\n",
    "    self.set_parameters(global_model)\n",
    "    self.model.train()\n",
    "\n",
    "    max_local_steps = self.local_steps\n",
    "\n",
    "    for step in range(max_local_steps):\n",
    "      for i, (x, y) in enumerate(trainloader):\n",
    "        # if type(x) == type([]):\n",
    "        #   x[0] = x[0].to(self.device)\n",
    "        # else:\n",
    "        #   x = x.to(self.device)\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        output = self.model(x)\n",
    "        output = output.view(-1, output.size(-1))  # Flatten output for cross-entropy\n",
    "        y = y.view(-1)  # Flatten target\n",
    "        # output = self.model(x)\n",
    "        loss = self.loss(output, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    model_update = [c_param.data - s_param.data for c_param, s_param in zip(self.model.parameters(), global_model.parameters())]\n",
    "    self.set_parameters(model)\n",
    "    return model_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the train_num and test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(object):\n",
    "  def __init__(self, model):\n",
    "    # Set up the main attributes\n",
    "    self.device = DEVICE\n",
    "    self.dataset = dataset\n",
    "    self.num_classes = input_size\n",
    "    self.global_rounds = global_rounds\n",
    "    self.local_steps = local_steps\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = local_learning_rate\n",
    "    self.global_model = copy.deepcopy(model)\n",
    "    self.num_clients = num_clients\n",
    "    self.join_ratio = join_ratio\n",
    "    self.attack_ratio = attack_ratio\n",
    "    self.attack_type = attack_type\n",
    "    self.seed = seed\n",
    "    self.algorithm = algorithm\n",
    "    self.current_round = -1\n",
    "    self.future_test = future_test\n",
    "    self.future_ratio = future_ratio\n",
    "    self.num_training_clients = num_clients - int(num_clients * future_ratio)\n",
    "    self.join_clients = int(self.num_training_clients * self.join_ratio)\n",
    "    self.finetune_rounds = finetune_rounds\n",
    "    self.eval_gap = eval_gap\n",
    "    self.detailed_info = detailed_info\n",
    "    self.partition = partition\n",
    "    self.data_path = data_path\n",
    "\n",
    "    self.clients = []\n",
    "    self.training_clients = []\n",
    "    self.malicious_ids = []\n",
    "    self.selected_clients = []\n",
    "\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_models = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    self.rs_test_recall_g = []\n",
    "    self.rs_test_mrr_g = []\n",
    "    self.rs_train_loss_g = []\n",
    "    self.rs_test_recalls_g = []\n",
    "    self.rs_test_mrrs_g = []\n",
    "    self.rs_test_recall_p = []\n",
    "    self.rs_test_mrr_p = []\n",
    "    self.rs_train_loss_p = []\n",
    "    self.rs_test_recalls_p = []\n",
    "    self.rs_test_mrrs_p = []\n",
    "    self.ft_train_loss = []\n",
    "    self.ft_test_recall = []\n",
    "    self.ft_std_recall = []\n",
    "    self.ft_test_mrr = []\n",
    "    self.ft_std_mrr = []\n",
    "\n",
    "  def set_clients(self, model, clientObj):\n",
    "\n",
    "    if self.future_test == False:\n",
    "      if self.attack_type == 'B':\n",
    "        self.malicious_ids = []\n",
    "        self.attack_ratio = 0.0\n",
    "      else:\n",
    "        self.malicious_ids = np.sort(np.random.choice(np.arange(self.num_clients), int(self.num_clients * self.attack_ratio), replace=False))\n",
    "\n",
    "\n",
    "      for i in range(self.num_clients):\n",
    "        client = clientObj(model=model, id=i,\n",
    "                        malicious=True if i in self.malicious_ids else False)\n",
    "        self.clients.append(client)\n",
    "\n",
    "      self.training_clients = self.clients\n",
    "      self.training_clients_ids = np.arange(self.num_clients)\n",
    "\n",
    "    else:\n",
    "      if self.algorithm != 'FedCHAR_DC':\n",
    "        print('{} do not support future testing'.format(self.algorithm))\n",
    "        raise NotImplementedError\n",
    "\n",
    "      self.training_clients_ids = np.sort(np.random.choice(np.arange(self.num_clients), self.num_training_clients, replace=False))\n",
    "\n",
    "      if self.attack_type == 'B':\n",
    "        self.malicious_ids = []\n",
    "        self.attack_ratio = 0.0\n",
    "      else:\n",
    "        self.malicious_ids = np.sort(np.random.choice(self.training_clients_ids, int(self.num_training_clients * self.attack_ratio),\n",
    "                                                      replace=False))\n",
    "\n",
    "      for i in range(self.num_clients):\n",
    "        client = clientObj(model=model, id=i,\n",
    "                        malicious=True if i in self.malicious_ids else False)\n",
    "        self.clients.append(client)\n",
    "\n",
    "        if i in self.training_clients_ids:\n",
    "          self.training_clients.append(client)\n",
    "\n",
    "    print('Malicious Clients: {}'.format(list(self.malicious_ids)))\n",
    "    print('Future Clients: {}'.format(list(np.sort(np.setdiff1d(np.arange(self.num_clients), self.training_clients_ids)))))\n",
    "\n",
    "  def select_clients(self):\n",
    "    selected_clients = list(np.random.choice(self.training_clients, self.join_clients, replace=False))\n",
    "    return selected_clients\n",
    "\n",
    "  def send_models(self):\n",
    "    for client in self.selected_clients:\n",
    "      client.set_parameters(self.global_model)\n",
    "\n",
    "  def send_models_to_future_clients(self):\n",
    "    for client in self.selected_clients:\n",
    "      client.set_parameters(self.global_model)\n",
    "\n",
    "  def receive_models(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = [] #weight based on the fraction of client's data\n",
    "    self.uploaded_models = []\n",
    "\n",
    "    tot_samples = 0\n",
    "    for client in self.selected_clients:\n",
    "      tot_samples += client.train_samples\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      self.uploaded_models.append(client.model)\n",
    "\n",
    "    for i, w in enumerate(self.uploaded_weights):\n",
    "      self.uploaded_weights[i] = w / tot_samples\n",
    "\n",
    "  def load_model(self):\n",
    "    model_path = os.path.join(f\"./models\", self.dataset)\n",
    "    model_path = os.path.join(model_path, self.algorithm + \"_server\" + \".pt\")\n",
    "    assert (os.path.exists(model_path))\n",
    "    self.global_model = torch.load(model_path)\n",
    "\n",
    "  def model_exists(self):\n",
    "    model_path = os.path.join(f\"./models\", self.dataset)\n",
    "    model_path = os.path.join(model_path, self.algorithm + \"_server\" + \".pt\")\n",
    "    return os.path.exists(model_path)\n",
    "\n",
    "  def save_results(self):\n",
    "    filename = \"{}_{}_{}_{}_{}_bz{}_lr{}_gr{}_ep{}_jr{}_nc{}_fur{}_ntc{}_ftr{}_seed{}\".format(self.dataset, self.partition, self.algorithm,\n",
    "                                                                                        self.attack_type, self.attack_ratio, self.batch_size,\n",
    "                                                                                        self.learning_rate, self.global_rounds, self.local_steps,\n",
    "                                                                                        self.join_ratio, self.num_clients, self.future_ratio,\n",
    "                                                                                        self.num_training_clients, self.finetune_rounds,\n",
    "                                                                                        self.seed)\n",
    "\n",
    "    if self.algorithm == 'FedCHAR':\n",
    "      filename = filename + '_ir{}_ng{}_mtrc{}_lkg{}'.format(self.initial_rounds, self.n_clusters, self.metric, self.linkage)\n",
    "\n",
    "    elif self.algorithm == 'FedCHAR_DC':\n",
    "      filename = filename + '_ir{}_ng{}_mtrc{}_lkg{}_rr{}'.format(self.initial_rounds, self.n_clusters, self.metric, self.linkage,\n",
    "                                                                  self.recluster_rounds)\n",
    "\n",
    "    result_path = f\"./results/npz/\"\n",
    "    if not os.path.exists(result_path):\n",
    "      os.makedirs(result_path)\n",
    "\n",
    "    if len(self.rs_test_acc_g) or len(self.rs_test_acc_p):\n",
    "      file_path = result_path + \"{}.npz\".format(filename)\n",
    "      print(\"Result path: \" + file_path)\n",
    "\n",
    "      np.savez(file_path, test_acc_g=self.rs_test_acc_g,\n",
    "              test_acc_p=self.rs_test_acc_p, test_accs_g=self.rs_test_accs_g,\n",
    "              test_accs_p=self.rs_test_accs_p, train_loss_g=self.rs_train_loss_g,\n",
    "              train_loss_p=self.rs_train_loss_p, ft_train_loss=self.ft_train_loss,\n",
    "              ft_test_acc=self.ft_test_acc, ft_std_acc=self.ft_std_acc)\n",
    "\n",
    "  # did not implemented the modification\n",
    "  def test_metrics_for_future_clients(self):\n",
    "    num_samples = []\n",
    "    tot_correct = []\n",
    "\n",
    "    for c in self.selected_clients:\n",
    "      ct, ns = c.new_test_metrics()\n",
    "      tot_correct.append(ct*1.0)\n",
    "      num_samples.append(ns)\n",
    "\n",
    "    ids = [c.id for c in self.selected_clients]\n",
    "    return ids, num_samples, tot_correct\n",
    "\n",
    "  # did not implemented the modification\n",
    "  def train_metrics_for_future_clients(self):\n",
    "    num_samples = []\n",
    "    losses = []\n",
    "    for c in self.selected_clients:\n",
    "      cl, ns = c.new_train_metrics()\n",
    "      num_samples.append(ns)\n",
    "      losses.append(cl*1.0)\n",
    "\n",
    "    ids = [c.id for c in self.selected_clients]\n",
    "    return ids, num_samples, losses\n",
    "\n",
    "  def evaluate_personalized(self, rec=None, loss=None, mrr=None):\n",
    "    stats = self.test_metrics_personalized()\n",
    "    stats_train = self.train_metrics_personalized()\n",
    "\n",
    "    if self.malicious_ids != []: # skip this for now\n",
    "      relative_malicious_ids = np.array([stats[0].index(i) for i in self.malicious_ids])\n",
    "\n",
    "      stats_A = np.array(stats)[:, relative_malicious_ids].tolist()\n",
    "      stats_train_A = np.array(stats_train)[:, relative_malicious_ids].tolist()\n",
    "\n",
    "      test_acc_A = sum(stats_A[2])*1.0 / sum(stats_A[1])\n",
    "      train_loss_A = sum(stats_train_A[2])*1.0 / sum(stats_train_A[1])\n",
    "      accs_A = [a / n for a, n in zip(stats_A[2], stats_A[1])]\n",
    "      losses_A = [a / n for a, n in zip(stats_train_A[2], stats_train_A[1])]\n",
    "\n",
    "    else:\n",
    "      test_acc_A = -1\n",
    "      train_loss_A = -1\n",
    "      accs_A = []\n",
    "      losses_A = []\n",
    "\n",
    "    benign_ids = np.sort(np.setdiff1d(self.training_clients_ids, self.malicious_ids))\n",
    "    relative_benign_ids = np.array([stats[0].index(i) for i in benign_ids])\n",
    "\n",
    "    stats_B = np.array(stats)[:, relative_benign_ids].tolist()\n",
    "    stats_train_B = np.array(stats_train)[:, relative_benign_ids].tolist()\n",
    "\n",
    "    stats = None\n",
    "    stats_train = None\n",
    "\n",
    "    # test_acc = sum(stats_B[2])*1.0 / sum(stats_B[1])\n",
    "    # train_loss = sum(stats_train_B[2])*1.0 / sum(stats_train_B[1])\n",
    "    # accs = [a / n for a, n in zip(stats_B[2], stats_B[1])]\n",
    "    # losses = [a / n for a, n in zip(stats_train_B[2], stats_train_B[1])]\n",
    "\n",
    "    test_recall = sum(stats_B[2])*1.0 / sum(stats_B[1])\n",
    "    test_mrr = sum(stats_B[3])*1.0 / sum(stats_B[1])\n",
    "    train_loss = sum(stats_train_B[2])*1.0 / sum(stats_train_B[1])\n",
    "    recalls = [a / n for a, n in zip(stats_B[2], stats_B[1])]\n",
    "    mrrs = [a / n for a, n in zip(stats_B[3], stats_B[1])]\n",
    "    losses = [a / n for a, n in zip(stats_train_B[2], stats_train_B[1])]\n",
    "\n",
    "    if rec == None:\n",
    "      self.rs_test_recall_p.append(test_recall)\n",
    "    else:\n",
    "      rec.append(test_recall)\n",
    "\n",
    "    if mrr == None:\n",
    "      self.rs_test_mrr_p.append(test_mrr)\n",
    "    else:\n",
    "      mrr.append(test_mrr)\n",
    "\n",
    "    if loss == None:\n",
    "      self.rs_train_loss_p.append(train_loss)\n",
    "    else:\n",
    "      loss.append(train_loss)\n",
    "\n",
    "    self.rs_test_recall_p.append(recalls)\n",
    "    self.rs_test_mrr_p.append(mrrs)\n",
    "\n",
    "    print(\"Benign Averaged Train Loss: {:.2f}\".format(train_loss))\n",
    "    # print(\"Benign Averaged Test Accurancy: {:.2f}%\".format(test_acc*100))\n",
    "    # print(\"Benign Std Test Accurancy: {:.2f}%\".format(np.std(accs)*100))\n",
    "    print(\"Benign Averaged Test Recall: {:.2f}%\".format(test_recall*100))\n",
    "    print(\"Benign Std Test Recall: {:.2f}%\".format(np.std(recalls)*100))\n",
    "    print(\"Benign Averaged Test MRR: {:.2f}%\".format(test_mrr*100))\n",
    "    print(\"Benign Std Test MRR: {:.2f}%\".format(np.std(mrrs)*100))\n",
    "\n",
    "    if self.malicious_ids != []:\n",
    "      print(\"Malicious Averaged Train Loss: {:.2f}\".format(train_loss_A))\n",
    "      print(\"Malicious Averaged Test Accurancy: {:.2f}%\".format(test_acc_A*100))\n",
    "\n",
    "  # did not implemented the modification\n",
    "  def evaluate_for_future_clients(self):\n",
    "    stats = self.test_metrics_for_future_clients()\n",
    "    stats_train = self.train_metrics_for_future_clients()\n",
    "    stats = np.array(stats).tolist()\n",
    "    stats_train = np.array(stats_train).tolist()\n",
    "    test_acc = sum(stats[2])*1.0 / sum(stats[1])\n",
    "    train_loss = sum(stats_train[2])*1.0 / sum(stats_train[1])\n",
    "    accs = [a / n for a, n in zip(stats[2], stats[1])]\n",
    "    losses = [a / n for a, n in zip(stats_train[2], stats_train[1])]\n",
    "\n",
    "    print(\"Averaged Future Train Loss: {:.2f}\".format(train_loss))\n",
    "    print(\"Averaged Future Test Accurancy: {:.2f}%\".format(test_acc*100))\n",
    "    print(\"Std Future Test Accurancy: {:.2f}%\".format(np.std(accs)*100))\n",
    "\n",
    "    if self.detailed_info:\n",
    "      print('Future Clients Train Loss:\\n', [(int(stats[0][idx]), format(loss, '.2f')) for idx, loss in enumerate(losses)])\n",
    "      print('Future Clients Test Accuracy:\\n', [(int(stats[0][idx]), format(acc*100, '.2f')+'%') for idx, acc in enumerate(accs)])\n",
    "\n",
    "    self.ft_train_loss.append(train_loss)\n",
    "    self.ft_test_acc.append(test_acc)\n",
    "    self.ft_std_acc.append(np.std(accs))\n",
    "\n",
    "  def test_metrics_personalized(self):\n",
    "    num_samples = []\n",
    "    tot_recall = []\n",
    "    tot_mrr = []\n",
    "\n",
    "    for c in self.training_clients:\n",
    "      rc, mrr, ns = c.test_metrics_personalized()\n",
    "      tot_recall.append(rc)\n",
    "      tot_mrr.append(mrr)\n",
    "      num_samples.append(ns)\n",
    "\n",
    "    ids = [c.id for c in self.training_clients]\n",
    "    return ids, num_samples, tot_recall, tot_mrr\n",
    "\n",
    "  def train_metrics_personalized(self):\n",
    "    num_samples = []\n",
    "    losses = []\n",
    "    for c in self.training_clients:\n",
    "      cl, ns = c.train_metrics_personalized()\n",
    "      num_samples.append(ns)\n",
    "      losses.append(cl*1.0)\n",
    "\n",
    "    ids = [c.id for c in self.training_clients]\n",
    "    return ids, num_samples, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedCHAR(Server):\n",
    "  def __init__(self, model):\n",
    "    super().__init__(model)\n",
    "\n",
    "    self.set_clients(model, clientCHAR)\n",
    "\n",
    "    print(f\"\\nJoin ratio / total clients: {self.join_ratio} / {self.num_training_clients}\")\n",
    "    print(\"Finished creating server and clients.\")\n",
    "\n",
    "    self.initial_rounds = initial_rounds\n",
    "    self.n_clusters = n_clusters\n",
    "    self.metric = metric\n",
    "    self.linkage = linkage\n",
    "\n",
    "  def train(self):\n",
    "    # initial Stage\n",
    "    for i in range(self.initial_rounds):\n",
    "      self.selected_clients = self.select_clients()\n",
    "      self.send_models()\n",
    "\n",
    "      for client in self.selected_clients:\n",
    "        client.dtrain()\n",
    "\n",
    "      if i%self.eval_gap == 0:\n",
    "        print(f\"\\n-------------Round number: {i}-------------\")\n",
    "        print(\"\\nEvaluate personalized models for training clients.\")\n",
    "        self.evaluate_personalized()\n",
    "\n",
    "      self.receive_models()\n",
    "      self.aggregate_parameters()\n",
    "\n",
    "    # Clustering Stage\n",
    "    print(f\"\\n-------------Clustering-------------\")\n",
    "    clients_updates = self.collect()\n",
    "    self.cluster_identity = self.cluster(clients_updates)\n",
    "    cluster_info = [[('Malicious' if self.training_clients[idx].malicious else 'Benign', idx) for idx, g_id in enumerate(self.cluster_identity) if g_id == i] for i in range(max(self.cluster_identity)+1)]\n",
    "    for idx, info in enumerate(cluster_info):\n",
    "      print('Cluster {}: {}'.format(idx, info))\n",
    "\n",
    "    self.group_models = [copy.deepcopy(self.global_model)] * (max(self.cluster_identity) + 1)\n",
    "\n",
    "    # Remaining Stage\n",
    "    for i in range(self.global_rounds - self.initial_rounds):\n",
    "      self.selected_clients = self.select_clients()\n",
    "      self.send_models_g()\n",
    "\n",
    "      for client in self.selected_clients:\n",
    "        client.dtrain()\n",
    "\n",
    "      if i%self.eval_gap == 0:\n",
    "        print(f\"\\n-------------Round number: {i+self.initial_rounds}-------------\")\n",
    "        print(\"\\nEvaluate personalized models for training clients.\")\n",
    "        self.evaluate_personalized()\n",
    "\n",
    "      self.receive_models_g()\n",
    "      self.aggregate_parameters_g()\n",
    "\n",
    "    print(\"\\nFinal Average Personalized Recall: {}\\n\".format(self.rs_test_recall_p[-1]))\n",
    "    print(f\"Average Recall for All Users: {np.mean(self.rs_test_recall_p[-1])}\")\n",
    "    print(\"\\nFinal Average Personalized Recall: {}\\n\".format(self.rs_test_mrr_p[-1]))\n",
    "    print(f\"Average MRR for All Users: {np.mean(self.rs_test_mrr_p[-1])}\")\n",
    "\n",
    "  def receive_models(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    tot_samples = 0\n",
    "    for client in self.selected_clients:\n",
    "      tot_samples += client.train_samples\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      self.uploaded_updates.append([c_param.data - s_param.data for c_param, s_param in zip(client.model.parameters(), self.global_model.parameters())])\n",
    "\n",
    "    if self.attack_type != 'B' and self.attack_type != 'A1':\n",
    "      malicious_ids = [idx for idx, c_id in enumerate(self.uploaded_ids) if c_id in self.malicious_ids]\n",
    "      self.uploaded_updates = eval(self.attack_type)(self.uploaded_updates, malicious_ids)\n",
    "\n",
    "    for i, w in enumerate(self.uploaded_weights):\n",
    "      self.uploaded_weights[i] = w / tot_samples\n",
    "\n",
    "  def add_parameters(self, w, client_update):\n",
    "    for server_param, client_param in zip(self.global_update, client_update):\n",
    "      server_param.data += client_param.data.clone() * w\n",
    "\n",
    "  def aggregate_parameters(self):\n",
    "    self.global_update = copy.deepcopy(self.uploaded_updates[0])\n",
    "    for param in self.global_update:\n",
    "      param.data.zero_()\n",
    "\n",
    "    for w, client_update in zip(self.uploaded_weights, self.uploaded_updates):\n",
    "      self.add_parameters(w, client_update)\n",
    "\n",
    "    for model_param, update_param in zip(self.global_model.parameters(), self.global_update):\n",
    "      model_param.data += update_param.data.clone()\n",
    "\n",
    "  def collect(self):\n",
    "    clients_updates = []\n",
    "    for client in self.training_clients:\n",
    "      clients_updates.append(client.get_update(self.global_model))\n",
    "\n",
    "    if self.attack_type != 'B' and self.attack_type != 'A1':\n",
    "      malicious_ids = [idx for idx, c_id in enumerate(self.training_clients_ids) if c_id in self.malicious_ids]\n",
    "      clients_updates = eval(self.attack_type)(clients_updates, malicious_ids, len(self.selected_clients))\n",
    "\n",
    "    clients_updates = [torch.cat([uu.reshape(-1, 1) for uu in u], axis=0).detach().cpu().numpy().squeeze() for u in clients_updates]\n",
    "    return clients_updates\n",
    "\n",
    "  def cluster(self, clients_updates):\n",
    "    clustering = AgglomerativeClustering(n_clusters=self.n_clusters, metric=self.metric, linkage=self.linkage).fit(clients_updates)\n",
    "    return clustering.labels_\n",
    "\n",
    "  def send_models_g(self):\n",
    "    for client in self.selected_clients:\n",
    "      c_idx = list(self.training_clients_ids).index(client.id)\n",
    "      client.set_parameters(self.group_models[self.cluster_identity[c_idx]])\n",
    "\n",
    "  def receive_models_g(self):\n",
    "    self.uploaded_ids = []\n",
    "    self.uploaded_weights = []\n",
    "    self.uploaded_updates = []\n",
    "\n",
    "    for client in self.selected_clients:\n",
    "      self.uploaded_ids.append(client.id)\n",
    "      self.uploaded_weights.append(client.train_samples)\n",
    "      c_idx = list(self.training_clients_ids).index(client.id)\n",
    "      self.uploaded_updates.append([c_param.data - s_param.data for c_param, s_param in zip(client.model.parameters(), self.group_models[self.cluster_identity[c_idx]].parameters())])\n",
    "\n",
    "    if self.attack_type != 'B' and self.attack_type != 'A1':\n",
    "      malicious_ids = [idx for idx, c_id in enumerate(self.uploaded_ids) if c_id in self.malicious_ids]\n",
    "      self.uploaded_updates = eval(self.attack_type)(self.uploaded_updates, malicious_ids)\n",
    "\n",
    "  def aggregate_parameters_g(self):\n",
    "    for i in range(len(self.group_models)):\n",
    "      self.global_update = copy.deepcopy(self.uploaded_updates[0])\n",
    "      for param in self.global_update:\n",
    "        param.data.zero_()\n",
    "\n",
    "      user_idx_in_same_group = np.array([r_id for r_id, c_id in enumerate(self.uploaded_ids) if self.cluster_identity[list(self.training_clients_ids).index(c_id)] == i])\n",
    "      uploaded_weights = [self.uploaded_weights[u_id] for u_id in range(len(self.uploaded_weights)) if u_id in user_idx_in_same_group]\n",
    "      uploaded_weights = [weight / sum(uploaded_weights) for weight in uploaded_weights]\n",
    "      uploaded_updates = [self.uploaded_updates[u_id] for u_id in range(len(self.uploaded_updates)) if u_id in user_idx_in_same_group]\n",
    "\n",
    "      for w, client_update in zip(uploaded_weights, uploaded_updates):\n",
    "        self.add_parameters(w, client_update)\n",
    "\n",
    "      for model_param, update_param in zip(self.group_models[i].parameters(), self.global_update):\n",
    "        model_param.data += update_param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating server and clients ...\n",
      "TransformerModel(\n",
      "  (embedding): Embedding(889, 400)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=400, out_features=400, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=400, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=400, bias=True)\n",
      "        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=400, out_features=889, bias=True)\n",
      ")\n",
      "Malicious Clients: []\n",
      "Future Clients: []\n",
      "\n",
      "Join ratio / total clients: 1.0 / 45\n",
      "Finished creating server and clients.\n",
      "\n",
      "-------------Round number: 0-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 6.51\n",
      "Benign Averaged Test Recall: 6.80%\n",
      "Benign Std Test Recall: 17.12%\n",
      "Benign Averaged Test MRR: 4.21%\n",
      "Benign Std Test MRR: 16.36%\n",
      "\n",
      "-------------Round number: 1-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 6.15\n",
      "Benign Averaged Test Recall: 13.59%\n",
      "Benign Std Test Recall: 25.33%\n",
      "Benign Averaged Test MRR: 8.90%\n",
      "Benign Std Test MRR: 23.07%\n",
      "\n",
      "-------------Round number: 2-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 5.82\n",
      "Benign Averaged Test Recall: 24.27%\n",
      "Benign Std Test Recall: 41.77%\n",
      "Benign Averaged Test MRR: 13.45%\n",
      "Benign Std Test MRR: 31.29%\n",
      "\n",
      "-------------Round number: 3-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 5.54\n",
      "Benign Averaged Test Recall: 26.21%\n",
      "Benign Std Test Recall: 41.82%\n",
      "Benign Averaged Test MRR: 18.53%\n",
      "Benign Std Test MRR: 35.21%\n",
      "\n",
      "-------------Round number: 4-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 5.21\n",
      "Benign Averaged Test Recall: 31.07%\n",
      "Benign Std Test Recall: 45.88%\n",
      "Benign Averaged Test MRR: 19.43%\n",
      "Benign Std Test MRR: 34.90%\n",
      "\n",
      "-------------Round number: 5-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 5.00\n",
      "Benign Averaged Test Recall: 33.01%\n",
      "Benign Std Test Recall: 46.31%\n",
      "Benign Averaged Test MRR: 21.72%\n",
      "Benign Std Test MRR: 36.96%\n",
      "\n",
      "-------------Round number: 6-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 4.76\n",
      "Benign Averaged Test Recall: 36.89%\n",
      "Benign Std Test Recall: 45.93%\n",
      "Benign Averaged Test MRR: 23.30%\n",
      "Benign Std Test MRR: 38.69%\n",
      "\n",
      "-------------Round number: 7-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 4.55\n",
      "Benign Averaged Test Recall: 39.81%\n",
      "Benign Std Test Recall: 46.77%\n",
      "Benign Averaged Test MRR: 24.82%\n",
      "Benign Std Test MRR: 38.47%\n",
      "\n",
      "-------------Round number: 8-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 4.41\n",
      "Benign Averaged Test Recall: 38.83%\n",
      "Benign Std Test Recall: 47.01%\n",
      "Benign Averaged Test MRR: 23.30%\n",
      "Benign Std Test MRR: 38.96%\n",
      "\n",
      "-------------Round number: 9-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 4.23\n",
      "Benign Averaged Test Recall: 47.57%\n",
      "Benign Std Test Recall: 45.77%\n",
      "Benign Averaged Test MRR: 29.05%\n",
      "Benign Std Test MRR: 37.75%\n",
      "\n",
      "-------------Clustering-------------\n",
      "Cluster 0: [('Benign', 0), ('Benign', 2), ('Benign', 3), ('Benign', 4), ('Benign', 5), ('Benign', 6), ('Benign', 7), ('Benign', 8), ('Benign', 12), ('Benign', 15), ('Benign', 16), ('Benign', 18), ('Benign', 19), ('Benign', 20), ('Benign', 21), ('Benign', 23), ('Benign', 27), ('Benign', 28), ('Benign', 29), ('Benign', 30), ('Benign', 31), ('Benign', 32), ('Benign', 33), ('Benign', 34), ('Benign', 36), ('Benign', 37), ('Benign', 38), ('Benign', 40), ('Benign', 41), ('Benign', 42), ('Benign', 44)]\n",
      "Cluster 1: [('Benign', 9), ('Benign', 11), ('Benign', 13), ('Benign', 17), ('Benign', 22), ('Benign', 24), ('Benign', 35), ('Benign', 39), ('Benign', 43)]\n",
      "Cluster 2: [('Benign', 1), ('Benign', 10), ('Benign', 14), ('Benign', 25), ('Benign', 26)]\n",
      "\n",
      "-------------Round number: 10-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 4.09\n",
      "Benign Averaged Test Recall: 40.78%\n",
      "Benign Std Test Recall: 45.95%\n",
      "Benign Averaged Test MRR: 26.17%\n",
      "Benign Std Test MRR: 38.38%\n",
      "\n",
      "-------------Round number: 11-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.91\n",
      "Benign Averaged Test Recall: 43.69%\n",
      "Benign Std Test Recall: 45.69%\n",
      "Benign Averaged Test MRR: 26.97%\n",
      "Benign Std Test MRR: 38.50%\n",
      "\n",
      "-------------Round number: 12-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.82\n",
      "Benign Averaged Test Recall: 47.57%\n",
      "Benign Std Test Recall: 44.72%\n",
      "Benign Averaged Test MRR: 29.39%\n",
      "Benign Std Test MRR: 37.76%\n",
      "\n",
      "-------------Round number: 13-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.72\n",
      "Benign Averaged Test Recall: 43.69%\n",
      "Benign Std Test Recall: 45.37%\n",
      "Benign Averaged Test MRR: 26.47%\n",
      "Benign Std Test MRR: 37.43%\n",
      "\n",
      "-------------Round number: 14-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.63\n",
      "Benign Averaged Test Recall: 46.60%\n",
      "Benign Std Test Recall: 45.18%\n",
      "Benign Averaged Test MRR: 30.89%\n",
      "Benign Std Test MRR: 37.51%\n",
      "\n",
      "-------------Round number: 15-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.50\n",
      "Benign Averaged Test Recall: 50.49%\n",
      "Benign Std Test Recall: 44.50%\n",
      "Benign Averaged Test MRR: 33.80%\n",
      "Benign Std Test MRR: 37.06%\n",
      "\n",
      "-------------Round number: 16-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.42\n",
      "Benign Averaged Test Recall: 48.54%\n",
      "Benign Std Test Recall: 44.64%\n",
      "Benign Averaged Test MRR: 30.73%\n",
      "Benign Std Test MRR: 37.14%\n",
      "\n",
      "-------------Round number: 17-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.37\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.36%\n",
      "Benign Std Test MRR: 37.65%\n",
      "\n",
      "-------------Round number: 18-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.27\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.88%\n",
      "Benign Std Test MRR: 37.94%\n",
      "\n",
      "-------------Round number: 19-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.22\n",
      "Benign Averaged Test Recall: 50.49%\n",
      "Benign Std Test Recall: 44.46%\n",
      "Benign Averaged Test MRR: 34.56%\n",
      "Benign Std Test MRR: 37.91%\n",
      "\n",
      "-------------Round number: 20-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.14\n",
      "Benign Averaged Test Recall: 48.54%\n",
      "Benign Std Test Recall: 45.03%\n",
      "Benign Averaged Test MRR: 31.49%\n",
      "Benign Std Test MRR: 38.29%\n",
      "\n",
      "-------------Round number: 21-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.09\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.39%\n",
      "Benign Std Test MRR: 37.59%\n",
      "\n",
      "-------------Round number: 22-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 3.04\n",
      "Benign Averaged Test Recall: 56.31%\n",
      "Benign Std Test Recall: 44.37%\n",
      "Benign Averaged Test MRR: 34.53%\n",
      "Benign Std Test MRR: 37.96%\n",
      "\n",
      "-------------Round number: 23-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.99\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.35%\n",
      "Benign Averaged Test MRR: 35.73%\n",
      "Benign Std Test MRR: 37.69%\n",
      "\n",
      "-------------Round number: 24-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.97\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.35%\n",
      "Benign Std Test MRR: 37.85%\n",
      "\n",
      "-------------Round number: 25-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.89\n",
      "Benign Averaged Test Recall: 47.57%\n",
      "Benign Std Test Recall: 44.69%\n",
      "Benign Averaged Test MRR: 30.65%\n",
      "Benign Std Test MRR: 37.67%\n",
      "\n",
      "-------------Round number: 26-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.88\n",
      "Benign Averaged Test Recall: 55.34%\n",
      "Benign Std Test Recall: 44.31%\n",
      "Benign Averaged Test MRR: 34.87%\n",
      "Benign Std Test MRR: 37.66%\n",
      "\n",
      "-------------Round number: 27-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.83\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 33.66%\n",
      "Benign Std Test MRR: 37.64%\n",
      "\n",
      "-------------Round number: 28-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.77\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.35%\n",
      "Benign Averaged Test MRR: 36.75%\n",
      "Benign Std Test MRR: 37.72%\n",
      "\n",
      "-------------Round number: 29-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.77\n",
      "Benign Averaged Test Recall: 50.49%\n",
      "Benign Std Test Recall: 44.46%\n",
      "Benign Averaged Test MRR: 34.72%\n",
      "Benign Std Test MRR: 37.37%\n",
      "\n",
      "-------------Round number: 30-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.72\n",
      "Benign Averaged Test Recall: 52.43%\n",
      "Benign Std Test Recall: 44.45%\n",
      "Benign Averaged Test MRR: 32.61%\n",
      "Benign Std Test MRR: 37.76%\n",
      "\n",
      "-------------Round number: 31-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.68\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.64%\n",
      "Benign Std Test MRR: 37.43%\n",
      "\n",
      "-------------Round number: 32-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.68\n",
      "Benign Averaged Test Recall: 52.43%\n",
      "Benign Std Test Recall: 44.73%\n",
      "Benign Averaged Test MRR: 31.78%\n",
      "Benign Std Test MRR: 38.06%\n",
      "\n",
      "-------------Round number: 33-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.62\n",
      "Benign Averaged Test Recall: 44.66%\n",
      "Benign Std Test Recall: 45.18%\n",
      "Benign Averaged Test MRR: 29.06%\n",
      "Benign Std Test MRR: 38.60%\n",
      "\n",
      "-------------Round number: 34-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.60\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.49%\n",
      "Benign Averaged Test MRR: 32.23%\n",
      "Benign Std Test MRR: 37.53%\n",
      "\n",
      "-------------Round number: 35-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.56\n",
      "Benign Averaged Test Recall: 57.28%\n",
      "Benign Std Test Recall: 44.52%\n",
      "Benign Averaged Test MRR: 35.58%\n",
      "Benign Std Test MRR: 37.96%\n",
      "\n",
      "-------------Round number: 36-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.51\n",
      "Benign Averaged Test Recall: 48.54%\n",
      "Benign Std Test Recall: 44.86%\n",
      "Benign Averaged Test MRR: 32.70%\n",
      "Benign Std Test MRR: 38.09%\n",
      "\n",
      "-------------Round number: 37-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.53\n",
      "Benign Averaged Test Recall: 48.54%\n",
      "Benign Std Test Recall: 44.86%\n",
      "Benign Averaged Test MRR: 33.43%\n",
      "Benign Std Test MRR: 38.31%\n",
      "\n",
      "-------------Round number: 38-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.47\n",
      "Benign Averaged Test Recall: 56.31%\n",
      "Benign Std Test Recall: 44.80%\n",
      "Benign Averaged Test MRR: 34.74%\n",
      "Benign Std Test MRR: 38.03%\n",
      "\n",
      "-------------Round number: 39-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.48\n",
      "Benign Averaged Test Recall: 50.49%\n",
      "Benign Std Test Recall: 44.46%\n",
      "Benign Averaged Test MRR: 32.65%\n",
      "Benign Std Test MRR: 37.82%\n",
      "\n",
      "-------------Round number: 40-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.45\n",
      "Benign Averaged Test Recall: 53.40%\n",
      "Benign Std Test Recall: 44.75%\n",
      "Benign Averaged Test MRR: 35.99%\n",
      "Benign Std Test MRR: 38.22%\n",
      "\n",
      "-------------Round number: 41-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.42\n",
      "Benign Averaged Test Recall: 52.43%\n",
      "Benign Std Test Recall: 44.70%\n",
      "Benign Averaged Test MRR: 32.69%\n",
      "Benign Std Test MRR: 37.89%\n",
      "\n",
      "-------------Round number: 42-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.43\n",
      "Benign Averaged Test Recall: 56.31%\n",
      "Benign Std Test Recall: 44.80%\n",
      "Benign Averaged Test MRR: 35.34%\n",
      "Benign Std Test MRR: 38.12%\n",
      "\n",
      "-------------Round number: 43-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.35\n",
      "Benign Averaged Test Recall: 56.31%\n",
      "Benign Std Test Recall: 44.80%\n",
      "Benign Averaged Test MRR: 35.44%\n",
      "Benign Std Test MRR: 38.01%\n",
      "\n",
      "-------------Round number: 44-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.38\n",
      "Benign Averaged Test Recall: 51.46%\n",
      "Benign Std Test Recall: 44.92%\n",
      "Benign Averaged Test MRR: 31.96%\n",
      "Benign Std Test MRR: 38.34%\n",
      "\n",
      "-------------Round number: 45-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.35\n",
      "Benign Averaged Test Recall: 44.66%\n",
      "Benign Std Test Recall: 45.18%\n",
      "Benign Averaged Test MRR: 28.79%\n",
      "Benign Std Test MRR: 38.27%\n",
      "\n",
      "-------------Round number: 46-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.32\n",
      "Benign Averaged Test Recall: 49.51%\n",
      "Benign Std Test Recall: 44.70%\n",
      "Benign Averaged Test MRR: 33.19%\n",
      "Benign Std Test MRR: 37.91%\n",
      "\n",
      "-------------Round number: 47-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.33\n",
      "Benign Averaged Test Recall: 46.60%\n",
      "Benign Std Test Recall: 44.82%\n",
      "Benign Averaged Test MRR: 30.73%\n",
      "Benign Std Test MRR: 37.62%\n",
      "\n",
      "-------------Round number: 48-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.31\n",
      "Benign Averaged Test Recall: 52.43%\n",
      "Benign Std Test Recall: 44.70%\n",
      "Benign Averaged Test MRR: 33.50%\n",
      "Benign Std Test MRR: 38.00%\n",
      "\n",
      "-------------Round number: 49-------------\n",
      "\n",
      "Evaluate personalized models for training clients.\n",
      "Benign Averaged Train Loss: 2.29\n",
      "Benign Averaged Test Recall: 49.51%\n",
      "Benign Std Test Recall: 44.64%\n",
      "Benign Averaged Test MRR: 35.18%\n",
      "Benign Std Test MRR: 37.97%\n",
      "\n",
      "Final Average Personalized Recall: [1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.46153846153846156, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "Average Recall for All Users: 0.5258648758648757\n",
      "\n",
      "Final Average Personalized Recall: [0.625, 0.4444444444444444, 0.75, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.8333333333333334, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.16666666666666666, 0.3333333333333333, 0.2, 0.0, 0.4102564102564103, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.5, 0.125, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0]\n",
      "\n",
      "Average MRR for All Users: 0.3786758920092253\n",
      "\n",
      "Time cost: 13.23min.\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "print(\"Creating server and clients ...\")\n",
    "start = time.time()\n",
    "# model = HARCNN(in_channels=3, num_classes=num_classes, dim=3008).to(device)\n",
    "model = TransformerModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers, nhead=nhead, dropout=0.0).to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "\n",
    "server = FedCHAR(model)\n",
    "server.train()\n",
    "# server.save_results()\n",
    "print(f\"\\nTime cost: {round((time.time()-start)/60, 2)}min.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
