{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from torch.nn import Module, Parameter\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diginetica\n"
     ]
    }
   ],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "      self.dataset = 'diginetica'\n",
    "      self.batchSize = 32\n",
    "      self.hiddenSize = 96\n",
    "      self.nhead = 2\n",
    "      self.layer = 1\n",
    "      self.feedforward = 4\n",
    "      self.epoch = 12\n",
    "      self.lr = 0.001\n",
    "      self.lr_dc = 0.1\n",
    "      self.lr_dc_step = 3\n",
    "      self.l2 = 1e-5\n",
    "      self.patience = 12\n",
    "\n",
    "opt = Options()\n",
    "\n",
    "# Now you can access parameters like this:\n",
    "print(opt.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "7\n",
      "16\n",
      "len train: 4\n",
      "len validation: 4\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "valid_data = []\n",
    "\n",
    "for i in range(45):\n",
    "  if os.path.isfile(f'./SR_SAN_Diginetica/train_{i}.txt'):\n",
    "    print(i)\n",
    "    tr_data = pickle.load(open(f'./SR_SAN_Diginetica/train_{i}.txt', 'rb'))\n",
    "    ts_data = pickle.load(open(f'./SR_SAN_Diginetica/test_{i}.txt', 'rb'))\n",
    "\n",
    "    train_data.append(tr_data)\n",
    "    valid_data.append(ts_data)\n",
    "\n",
    "print(f\"len train: {len(train_data)}\")\n",
    "print(f\"len validation: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_masks(all_usr_pois, item_tail):\n",
    "#   us_lens = [len(upois) for upois in all_usr_pois]\n",
    "#   len_max = max(us_lens)\n",
    "#   us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "#   us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "#   return us_pois, us_msks, len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_masks(all_usr_pois, item_tail):\n",
    "    if not all_usr_pois or all(len(upois) == 0 for upois in all_usr_pois):\n",
    "        raise ValueError(\"Input all_usr_pois is empty or contains only empty lists\")\n",
    "\n",
    "    us_lens = [len(upois) for upois in all_usr_pois]\n",
    "    len_max = max(us_lens)\n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "    return us_pois, us_msks, len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, data, shuffle=False, graph=None):\n",
    "      inputs = data[0]\n",
    "      inputs, mask, len_max = data_masks(inputs, [0])\n",
    "      self.inputs = np.asarray(inputs)\n",
    "      self.mask = np.asarray(mask)\n",
    "      self.len_max = len_max\n",
    "      self.targets = np.asarray(data[1])\n",
    "      self.length = len(inputs)\n",
    "      self.shuffle = shuffle\n",
    "      self.graph = graph\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "      if self.shuffle:\n",
    "        shuffled_arg = np.arange(self.length)\n",
    "        np.random.shuffle(shuffled_arg)\n",
    "        self.inputs = self.inputs[shuffled_arg]\n",
    "        self.mask = self.mask[shuffled_arg]\n",
    "        self.targets = self.targets[shuffled_arg]\n",
    "      n_batch = int(self.length / batch_size)\n",
    "      if self.length % batch_size != 0:\n",
    "        n_batch += 1\n",
    "      slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "      slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
    "      return slices\n",
    "\n",
    "    def get_slice(self, i):\n",
    "      inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
    "      items, n_node, A, alias_inputs = [], [], [], []\n",
    "      for u_input in inputs:\n",
    "        n_node.append(len(np.unique(u_input)))\n",
    "      max_n_node = np.max(n_node)\n",
    "      for u_input in inputs:\n",
    "        node = np.unique(u_input)\n",
    "        items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "        u_A = np.zeros((max_n_node, max_n_node))\n",
    "        for i in np.arange(len(u_input) - 1):\n",
    "          if u_input[i + 1] == 0:\n",
    "            break\n",
    "          u = np.where(node == u_input[i])[0][0]\n",
    "          v = np.where(node == u_input[i + 1])[0][0]\n",
    "          u_A[u][v] = 1\n",
    "        u_sum_in = np.sum(u_A, 0)\n",
    "        u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "        u_A_in = np.divide(u_A, u_sum_in)\n",
    "        u_sum_out = np.sum(u_A, 1)\n",
    "        u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "        u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "        u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "        A.append(u_A)\n",
    "        alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "      return alias_inputs, A, items, mask, targets\n",
    "\n",
    "    def __len__(self):\n",
    "      return self.length  # or return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionNetwork(Module):\n",
    "  def __init__(self, opt, n_node):\n",
    "    super(SelfAttentionNetwork, self).__init__()\n",
    "    self.hidden_size = opt.hiddenSize\n",
    "    self.n_node = n_node\n",
    "    self.batch_size = opt.batchSize\n",
    "    self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
    "    self.transformerEncoderLayer = TransformerEncoderLayer(d_model=self.hidden_size, nhead=opt.nhead,dim_feedforward=self.hidden_size * opt.feedforward)\n",
    "    self.transformerEncoder = TransformerEncoder(self.transformerEncoderLayer, opt.layer)\n",
    "    self.loss_function = nn.CrossEntropyLoss()\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
    "    self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "    for weight in self.parameters():\n",
    "      weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "  def compute_scores(self, hidden, mask):\n",
    "    ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
    "    b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
    "    scores = torch.matmul(ht, b.transpose(1, 0))\n",
    "    return scores\n",
    "\n",
    "  def forward(self, inputs, A):\n",
    "    hidden = self.embedding(inputs)\n",
    "    hidden = hidden.transpose(0,1).contiguous()\n",
    "    hidden = self.transformerEncoder(hidden)\n",
    "    hidden = hidden.transpose(0,1).contiguous()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_cuda(variable):\n",
    "  if torch.cuda.is_available():\n",
    "    return variable.cuda()\n",
    "  else:\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_cpu(variable):\n",
    "  if torch.cuda.is_available():\n",
    "    return variable.cpu()\n",
    "  else:\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, i, data):\n",
    "  alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
    "  alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "  items = trans_to_cuda(torch.Tensor(items).long())\n",
    "  A = trans_to_cuda(torch.Tensor(A).float())\n",
    "  mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "  hidden = model(items, A)\n",
    "  get = lambda i: hidden[i][alias_inputs[i]]\n",
    "  seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "  return targets, model.compute_scores(seq_hidden, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, device):\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        # targets = targets.to(device)  # Ensure targets are on the correct device\n",
    "        sub_scores = scores.topk(5)[1]\n",
    "        sub_scores = sub_scores.cpu().detach().numpy()  # Move to CPU if necessary\n",
    "\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            mrr_index = np.where(score == target - 1)[0]\n",
    "            mrr.append(0 if len(mrr_index) == 0 else 1 / (mrr_index[0] + 1))\n",
    "\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "\n",
    "    # model.scheduler.step()  # Typically used in training, not testing\n",
    "\n",
    "    return hit, mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, epochs, device):\n",
    "    print('start training: ', datetime.datetime.now())\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        slices = train_data.generate_batch(model.batch_size)\n",
    "        num_batches = len(slices)\n",
    "\n",
    "        for i, j in zip(slices, np.arange(num_batches)):\n",
    "            model.optimizer.zero_grad()\n",
    "            targets, scores = forward(model, i, train_data)\n",
    "            targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "            # targets = targets.to(device)  # Ensure targets are on the correct device\n",
    "            loss = model.loss_function(scores, targets - 1)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            total_loss += loss.item()  # Use .item() to get the scalar value\n",
    "\n",
    "            if j % int(num_batches / 5 + 1) == 0:\n",
    "                print(f'[{j}/{num_batches}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch: {epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        hit, mrr = test(model, test_data, device)\n",
    "\n",
    "        model.scheduler.step()\n",
    "    \n",
    "    return hit, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tori_rs\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training:  2023-12-04 02:36:27.021106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IME-LAB\\AppData\\Local\\Temp\\ipykernel_17276\\569402041.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  A = trans_to_cuda(torch.Tensor(A).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1] Loss: 6.8144\n",
      "Epoch: 0, Average Loss: 6.8144\n",
      "start predicting:  2023-12-04 02:36:28.683648\n",
      "[0/1] Loss: 6.7674\n",
      "Epoch: 1, Average Loss: 6.7674\n",
      "start predicting:  2023-12-04 02:36:28.800969\n",
      "[0/1] Loss: 6.7176\n",
      "Epoch: 2, Average Loss: 6.7176\n",
      "start predicting:  2023-12-04 02:36:28.811938\n",
      "[0/1] Loss: 6.6711\n",
      "Epoch: 3, Average Loss: 6.6711\n",
      "start predicting:  2023-12-04 02:36:28.820942\n",
      "[0/1] Loss: 6.6666\n",
      "Epoch: 4, Average Loss: 6.6666\n",
      "start predicting:  2023-12-04 02:36:28.829918\n",
      "[0/1] Loss: 6.6620\n",
      "Epoch: 5, Average Loss: 6.6620\n",
      "start predicting:  2023-12-04 02:36:28.838866\n",
      "[0/1] Loss: 6.6575\n",
      "Epoch: 6, Average Loss: 6.6575\n",
      "start predicting:  2023-12-04 02:36:28.848627\n",
      "[0/1] Loss: 6.6570\n",
      "Epoch: 7, Average Loss: 6.6570\n",
      "start predicting:  2023-12-04 02:36:28.857604\n",
      "[0/1] Loss: 6.6565\n",
      "Epoch: 8, Average Loss: 6.6565\n",
      "start predicting:  2023-12-04 02:36:28.867578\n",
      "[0/1] Loss: 6.6561\n",
      "Epoch: 9, Average Loss: 6.6561\n",
      "start predicting:  2023-12-04 02:36:28.876554\n",
      "[0/1] Loss: 6.6560\n",
      "Epoch: 10, Average Loss: 6.6560\n",
      "start predicting:  2023-12-04 02:36:28.886527\n",
      "[0/1] Loss: 6.6560\n",
      "Epoch: 11, Average Loss: 6.6560\n",
      "start predicting:  2023-12-04 02:36:28.896043\n",
      "Hit: 0.0000\n",
      "MRR: 0.0000\n",
      "Client 1\n",
      "start training:  2023-12-04 02:36:28.912019\n",
      "[0/1] Loss: 6.8354\n",
      "Epoch: 0, Average Loss: 6.8354\n",
      "start predicting:  2023-12-04 02:36:28.938947\n",
      "[0/1] Loss: 6.8007\n",
      "Epoch: 1, Average Loss: 6.8007\n",
      "start predicting:  2023-12-04 02:36:28.952909\n",
      "[0/1] Loss: 6.7609\n",
      "Epoch: 2, Average Loss: 6.7609\n",
      "start predicting:  2023-12-04 02:36:28.967870\n",
      "[0/1] Loss: 6.7229\n",
      "Epoch: 3, Average Loss: 6.7229\n",
      "start predicting:  2023-12-04 02:36:28.980835\n",
      "[0/1] Loss: 6.7191\n",
      "Epoch: 4, Average Loss: 6.7191\n",
      "start predicting:  2023-12-04 02:36:28.995795\n",
      "[0/1] Loss: 6.7153\n",
      "Epoch: 5, Average Loss: 6.7153\n",
      "start predicting:  2023-12-04 02:36:29.008761\n",
      "[0/1] Loss: 6.7115\n",
      "Epoch: 6, Average Loss: 6.7115\n",
      "start predicting:  2023-12-04 02:36:29.022724\n",
      "[0/1] Loss: 6.7111\n",
      "Epoch: 7, Average Loss: 6.7111\n",
      "start predicting:  2023-12-04 02:36:29.035688\n",
      "[0/1] Loss: 6.7108\n",
      "Epoch: 8, Average Loss: 6.7108\n",
      "start predicting:  2023-12-04 02:36:29.048653\n",
      "[0/1] Loss: 6.7104\n",
      "Epoch: 9, Average Loss: 6.7104\n",
      "start predicting:  2023-12-04 02:36:29.063614\n",
      "[0/1] Loss: 6.7103\n",
      "Epoch: 10, Average Loss: 6.7103\n",
      "start predicting:  2023-12-04 02:36:29.078573\n",
      "[0/1] Loss: 6.7103\n",
      "Epoch: 11, Average Loss: 6.7103\n",
      "start predicting:  2023-12-04 02:36:29.092537\n",
      "Hit: 0.0000\n",
      "MRR: 0.0000\n",
      "Client 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input all_usr_pois is empty or contains only empty lists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_srsan_diginetica.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClient \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_data_i \u001b[39m=\u001b[39m Data(train_data[i], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_data_i \u001b[39m=\u001b[39m Data(valid_data[i], shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m trans_to_cuda(SelfAttentionNetwork(opt, n_node))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m hit, mrr \u001b[39m=\u001b[39m train(model, train_data_i, test_data_i, opt\u001b[39m.\u001b[39mepoch, DEVICE)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_srsan_diginetica.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, graph\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   inputs \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   inputs, mask, len_max \u001b[39m=\u001b[39m data_masks(inputs, [\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(inputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(mask)\n",
      "\u001b[1;32md:\\session-based_rs_fl\\fl_srsan_diginetica.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_masks\u001b[39m(all_usr_pois, item_tail):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m all_usr_pois \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(upois) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m upois \u001b[39min\u001b[39;00m all_usr_pois):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput all_usr_pois is empty or contains only empty lists\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     us_lens \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(upois) \u001b[39mfor\u001b[39;00m upois \u001b[39min\u001b[39;00m all_usr_pois]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/session-based_rs_fl/fl_srsan_diginetica.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     len_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(us_lens)\n",
      "\u001b[1;31mValueError\u001b[0m: Input all_usr_pois is empty or contains only empty lists"
     ]
    }
   ],
   "source": [
    "hit_list = []\n",
    "mrr_list = []\n",
    "\n",
    "if opt.dataset == 'diginetica':\n",
    "  n_node = 889\n",
    "else:\n",
    "  n_node = 37484\n",
    "\n",
    "#iterate over all clients\n",
    "for i in range(len(train_data)):\n",
    "  print(f\"Client {i}\")\n",
    "  train_data_i = Data(train_data[i], shuffle=True)\n",
    "  test_data_i = Data(valid_data[i], shuffle=False)\n",
    "  model = trans_to_cuda(SelfAttentionNetwork(opt, n_node))\n",
    "  hit, mrr = train(model, train_data_i, test_data_i, opt.epoch, DEVICE)\n",
    "  hit_list.append(hit)\n",
    "  mrr_list.append(mrr)\n",
    "\n",
    "  #print hit and mrr for each client\n",
    "  print(f\"Hit: {hit:.4f}\")\n",
    "  print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "#print average hit and mrr over all clients\n",
    "print(f\"Average Hit: {np.mean(hit_list):.4f}\")\n",
    "print(f\"Average MRR: {np.mean(mrr_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
